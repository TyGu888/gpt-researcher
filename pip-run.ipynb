{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "byPgKYhAE6gn"
      },
      "outputs": [],
      "source": [
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KWZe2InrL0ji"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîé Starting the research task for ' What new research work was published in the past 30 days on image generation especially on disfussion based model?'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\Conda\\envs\\gptresearch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¨ Research Scholar Agent\n",
            "response :  [\"new research on diffusion based image generation models published in the past 30 days\", \"latest studies on diffusion models for image generation June 2024\", \"recent publications on image generation using diffusion models June 2024\"]\n",
            "üß† I will conduct my research based on the following queries: ['new research on diffusion based image generation models published in the past 30 days', 'latest studies on diffusion models for image generation June 2024', 'recent publications on image generation using diffusion models June 2024', ' What new research work was published in the past 30 days on image generation especially on disfussion based model?']...\n",
            "\n",
            "üîé Running research for 'new research on diffusion based image generation models published in the past 30 days'...\n",
            "‚úÖ Added source url to research: https://arxiv.org/pdf/2308.13142v1\n",
            "\n",
            "‚úÖ Added source url to research: https://ar5iv.labs.arxiv.org/html/2303.07909\n",
            "\n",
            "‚úÖ Added source url to research: https://ar5iv.labs.arxiv.org/html/2308.13142\n",
            "\n",
            "‚úÖ Added source url to research: https://arxiv.org/abs/2308.13142\n",
            "\n",
            "‚úÖ Added source url to research: https://ar5iv.labs.arxiv.org/html/2209.00796\n",
            "\n",
            "ü§î Researching for relevant information...\n",
            "\n",
            "arxiv\n",
            "arxiv\n",
            "arxiv\n",
            "arxiv\n",
            "arxiv\n",
            "üìù Getting relevant content based on query: new research on diffusion based image generation models published in the past 30 days...\n",
            "base_compressor=DocumentCompressorPipeline(transformers=[<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x000001865CA001D0>, EmbeddingsFilter(embeddings=OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001865AA0E090>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001865AA301D0>, model='text-embedding-3-small', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True), similarity_fn=<function cosine_similarity at 0x00000186583FFE20>, k=20, similarity_threshold=0.2)]) base_retriever=SearchAPIRetriever(pages=[{'url': 'https://arxiv.org/pdf/2308.13142v1', 'raw_content': \"Recently, there has been significant progress in the development of large\\nmodels. Following the success of ChatGPT, numerous language models have been\\nintroduced, demonstrating remarkable performance. Similar advancements have\\nalso been observed in image generation models, such as Google's Imagen model,\\nOpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive\\ncapabilities in generating images. However, similar to large language models,\\nthese models still encounter unresolved challenges. Fortunately, the\\navailability of open-source stable diffusion models and their underlying\\nmathematical principles has enabled the academic community to extensively\\nanalyze the performance of current image generation models and make\\nimprovements based on this stable diffusion framework. This survey aims to\\nexamine the existing issues and the current solutions pertaining to image\\ngeneration models.\", 'time': datetime.date(2023, 8, 25)}, {'url': 'https://ar5iv.labs.arxiv.org/html/2303.07909', 'raw_content': 'This survey reviews text-to-image diffusion models in the context that\\ndiffusion models have emerged to be popular for a wide range of generative\\ntasks. As a self-contained work, this survey starts with a brief introduction\\nof how a basic diffusion model works for image synthesis, followed by how\\ncondition or guidance improves learning. Based on that, we present a review of\\nstate-of-the-art methods on text-conditioned image synthesis, i.e.,\\ntext-to-image. We further summarize applications beyond text-to-image\\ngeneration: text-guided creative generation and text-guided image editing.\\nBeyond the progress made so far, we discuss existing challenges and promising\\nfuture directions.', 'time': datetime.date(2023, 4, 2)}, {'url': 'https://ar5iv.labs.arxiv.org/html/2308.13142', 'raw_content': \"Recently, there has been significant progress in the development of large\\nmodels. Following the success of ChatGPT, numerous language models have been\\nintroduced, demonstrating remarkable performance. Similar advancements have\\nalso been observed in image generation models, such as Google's Imagen model,\\nOpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive\\ncapabilities in generating images. However, similar to large language models,\\nthese models still encounter unresolved challenges. Fortunately, the\\navailability of open-source stable diffusion models and their underlying\\nmathematical principles has enabled the academic community to extensively\\nanalyze the performance of current image generation models and make\\nimprovements based on this stable diffusion framework. This survey aims to\\nexamine the existing issues and the current solutions pertaining to image\\ngeneration models.\", 'time': datetime.date(2023, 8, 25)}, {'url': 'https://arxiv.org/abs/2308.13142', 'raw_content': \"Recently, there has been significant progress in the development of large\\nmodels. Following the success of ChatGPT, numerous language models have been\\nintroduced, demonstrating remarkable performance. Similar advancements have\\nalso been observed in image generation models, such as Google's Imagen model,\\nOpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive\\ncapabilities in generating images. However, similar to large language models,\\nthese models still encounter unresolved challenges. Fortunately, the\\navailability of open-source stable diffusion models and their underlying\\nmathematical principles has enabled the academic community to extensively\\nanalyze the performance of current image generation models and make\\nimprovements based on this stable diffusion framework. This survey aims to\\nexamine the existing issues and the current solutions pertaining to image\\ngeneration models.\", 'time': datetime.date(2023, 8, 25)}, {'url': 'https://ar5iv.labs.arxiv.org/html/2209.00796', 'raw_content': 'Diffusion models have emerged as a powerful new family of deep generative\\nmodels with record-breaking performance in many applications, including image\\nsynthesis, video generation, and molecule design. In this survey, we provide an\\noverview of the rapidly expanding body of work on diffusion models,\\ncategorizing the research into three key areas: efficient sampling, improved\\nlikelihood estimation, and handling data with special structures. We also\\ndiscuss the potential for combining diffusion models with other generative\\nmodels for enhanced results. We further review the wide-ranging applications of\\ndiffusion models in fields spanning from computer vision, natural language\\ngeneration, temporal data modeling, to interdisciplinary applications in other\\nscientific disciplines. This survey aims to provide a contextualized, in-depth\\nlook at the state of diffusion models, identifying the key areas of focus and\\npointing to potential areas for further exploration. Github:\\nhttps://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.', 'time': datetime.date(2024, 2, 6)}])\n",
            "üìÉ Source: https://ar5iv.labs.arxiv.org/html/2209.00796\n",
            "Title: \n",
            "Published_time: 2024-02-06\n",
            "Content: Diffusion models have emerged as a powerful new family of deep generative\n",
            "models with record-breaking performance in many applications, including image\n",
            "synthesis, video generation, and molecule design. In this survey, we provide an\n",
            "overview of the rapidly expanding body of work on diffusion models,\n",
            "categorizing the research into three key areas: efficient sampling, improved\n",
            "likelihood estimation, and handling data with special structures. We also\n",
            "discuss the potential for combining diffusion models with other generative\n",
            "models for enhanced results. We further review the wide-ranging applications of\n",
            "diffusion models in fields spanning from computer vision, natural language\n",
            "generation, temporal data modeling, to interdisciplinary applications in other\n",
            "scientific disciplines. This survey aims to provide a contextualized, in-depth\n",
            "look at the state of diffusion models, identifying the key areas of focus and\n",
            "pointing to potential areas for further exploration. Github:\n",
            "\n",
            "Source: https://arxiv.org/abs/2308.13142\n",
            "Title: \n",
            "Published_time: 2023-08-25\n",
            "Content: Recently, there has been significant progress in the development of large\n",
            "models. Following the success of ChatGPT, numerous language models have been\n",
            "introduced, demonstrating remarkable performance. Similar advancements have\n",
            "also been observed in image generation models, such as Google's Imagen model,\n",
            "OpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive\n",
            "capabilities in generating images. However, similar to large language models,\n",
            "these models still encounter unresolved challenges. Fortunately, the\n",
            "availability of open-source stable diffusion models and their underlying\n",
            "mathematical principles has enabled the academic community to extensively\n",
            "analyze the performance of current image generation models and make\n",
            "improvements based on this stable diffusion framework. This survey aims to\n",
            "examine the existing issues and the current solutions pertaining to image\n",
            "generation models.\n",
            "\n",
            "Source: https://ar5iv.labs.arxiv.org/html/2308.13142\n",
            "Title: \n",
            "Published_time: 2023-08-25\n",
            "Content: Recently, there has been significant progress in the development of large\n",
            "models. Following the success of ChatGPT, numerous language models have been\n",
            "introduced, demonstrating remarkable performance. Similar advancements have\n",
            "also been observed in image generation models, such as Google's Imagen model,\n",
            "OpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive\n",
            "capabilities in generating images. However, similar to large language models,\n",
            "these models still encounter unresolved challenges. Fortunately, the\n",
            "availability of open-source stable diffusion models and their underlying\n",
            "mathematical principles has enabled the academic community to extensively\n",
            "analyze the performance of current image generation models and make\n",
            "improvements based on this stable diffusion framework. This survey aims to\n",
            "examine the existing issues and the current solutions pertaining to image\n",
            "generation models.\n",
            "\n",
            "Source: https://arxiv.org/pdf/2308.13142v1\n",
            "Title: \n",
            "Published_time: 2023-08-25\n",
            "Content: Recently, there has been significant progress in the development of large\n",
            "models. Following the success of ChatGPT, numerous language models have been\n",
            "introduced, demonstrating remarkable performance. Similar advancements have\n",
            "also been observed in image generation models, such as Google's Imagen model,\n",
            "OpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive\n",
            "capabilities in generating images. However, similar to large language models,\n",
            "these models still encounter unresolved challenges. Fortunately, the\n",
            "availability of open-source stable diffusion models and their underlying\n",
            "mathematical principles has enabled the academic community to extensively\n",
            "analyze the performance of current image generation models and make\n",
            "improvements based on this stable diffusion framework. This survey aims to\n",
            "examine the existing issues and the current solutions pertaining to image\n",
            "generation models.\n",
            "\n",
            "Source: https://ar5iv.labs.arxiv.org/html/2303.07909\n",
            "Title: \n",
            "Published_time: 2023-04-02\n",
            "Content: This survey reviews text-to-image diffusion models in the context that\n",
            "diffusion models have emerged to be popular for a wide range of generative\n",
            "tasks. As a self-contained work, this survey starts with a brief introduction\n",
            "of how a basic diffusion model works for image synthesis, followed by how\n",
            "condition or guidance improves learning. Based on that, we present a review of\n",
            "state-of-the-art methods on text-conditioned image synthesis, i.e.,\n",
            "text-to-image. We further summarize applications beyond text-to-image\n",
            "generation: text-guided creative generation and text-guided image editing.\n",
            "Beyond the progress made so far, we discuss existing challenges and promising\n",
            "future directions.\n",
            "\n",
            "Source: https://ar5iv.labs.arxiv.org/html/2209.00796\n",
            "Title: \n",
            "Published_time: 2024-02-06\n",
            "Content: pointing to potential areas for further exploration. Github:\n",
            "https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.\n",
            "\n",
            "\n",
            "üîé Running research for 'latest studies on diffusion models for image generation June 2024'...\n",
            "‚úÖ Added source url to research: https://news.mit.edu/2024/controlled-diffusion-model-can-change-material-properties-images-0528\n",
            "\n",
            "‚úÖ Added source url to research: https://news.mit.edu/2024/ai-generates-high-quality-images-30-times-faster-single-step-0321\n",
            "\n",
            "‚úÖ Added source url to research: https://scitechdaily.com/mits-new-generative-ai-outperforms-diffusion-models-in-image-generation/\n",
            "\n",
            "‚úÖ Added source url to research: https://github.com/cvlab-stonybrook/Large-Image-Diffusion\n",
            "\n",
            "‚úÖ Added source url to research: https://arxiv.org/abs/2406.02347\n",
            "\n",
            "ü§î Researching for relevant information...\n",
            "\n",
            "bs\n",
            "bs\n",
            "bs\n",
            "bs\n",
            "arxiv\n",
            "üìù Getting relevant content based on query: latest studies on diffusion models for image generation June 2024...\n",
            "base_compressor=DocumentCompressorPipeline(transformers=[<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x000001865C332950>, EmbeddingsFilter(embeddings=OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001865AA0E090>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001865AA301D0>, model='text-embedding-3-small', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True), similarity_fn=<function cosine_similarity at 0x00000186583FFE20>, k=20, similarity_threshold=0.2)]) base_retriever=SearchAPIRetriever(pages=[{'url': 'https://news.mit.edu/2024/controlled-diffusion-model-can-change-material-properties-images-0528', 'raw_content': 'Suggestions or feedback?\\nMIT News | Massachusetts Institute of Technology\\nBrowse By\\nTopics\\nDepartments\\nCenters, Labs, & Programs\\nSchools\\nBreadcrumb\\nControlled diffusion model can change material properties in images\\nPress Contact:\\nPrevious image\\nNext image\\nResearchers from the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and Google Research may have just performed digital sorcery ‚Äî in the form of a diffusion model that can change the material properties of objects in images.Dubbed Alchemist, the system allows users to alter four attributes of both real and AI-generated pictures: roughness, metallicity, albedo (an object‚Äôs initial base color), and transparency. As an image-to-image diffusion model, one can input any photo and then adjust each property within a continuous scale of -1 to 1 to create a new visual. These photo editing capabilities could potentially extend to improving the models in video games, expanding the capabilities of AI in visual effects, and enriching robotic training data.\\nThe magic behind Alchemist starts with a denoising diffusion model: In practice, researchers used Stable Diffusion 1.5, which is a text-to-image model lauded for its photorealistic results and editing capabilities. Previous work built on the popular model to enable users to make higher-level changes, like swapping objects or altering the depth of images. In contrast, CSAIL and Google Research‚Äôs method applies this model to focus on low-level attributes, revising the finer details of an object‚Äôs material properties with a unique, slider-based interface that outperforms its counterparts.While prior diffusion systems could pull a proverbial rabbit out of a hat for an image, Alchemist could transform that same animal to look translucent. The system could also make a rubber duck appear metallic, remove the golden hue from a goldfish, and shine an old shoe. Programs like Photoshop have similar capabilities, but this model can change material properties in a more straightforward way. For instance, modifying the metallic look of a photo requires several steps in the widely used application.\\n‚ÄúWhen you look at an image you‚Äôve created, often the result is not exactly what you have in mind,‚Äù says Prafull Sharma, MIT PhD student in electrical engineering and computer science, CSAIL affiliate, and lead author on a new paper describing the work. ‚ÄúYou want to control the picture while editing it, but the existing controls in image editors are not able to change the materials. With Alchemist, we capitalize on the photorealism of outputs from text-to-image models and tease out a slider control that allows us to modify a specific property after the initial picture is provided.‚Äù\\nPrecise control\\n‚ÄúText-to-image generative models have empowered everyday users to generate images as effortlessly as writing a sentence. However, controlling these models can be challenging,‚Äù says Carnegie Mellon University Assistant Professor Jun-Yan Zhu, who was not involved in the paper. ‚ÄúWhile generating a vase is simple, synthesizing a vase with specific material properties such as transparency and roughness requires users to spend hours trying different text prompts and random seeds. This can be frustrating, especially for professional users who require precision in their work. Alchemist presents a practical solution to this challenge by enabling precise control over the materials of an input image while harnessing the data-driven priors of large-scale diffusion models, inspiring future works to seamlessly incorporate generative models into the existing interfaces of commonly used content creation software.‚Äù\\nAlchemist‚Äôs design capabilities could help tweak the appearance of different models in video games. Applying such a diffusion model in this domain could help creators speed up their design process, refining textures to fit the gameplay of a level. Moreover, Sharma and his team‚Äôs project could assist with altering graphic design elements, videos, and movie effects to enhance photorealism and achieve the desired material appearance with precision.\\nThe method could also refine robotic training data for tasks like manipulation. By introducing the machines to more textures, they can better understand the diverse items they‚Äôll grasp in the real world. Alchemist can even potentially help with image classification, analyzing where a neural network fails to recognize the material changes of an image.\\nSharma and his team‚Äôs work exceeded similar models at faithfully editing only the requested object of interest. For example, when a user prompted different models to tweak a dolphin to max transparency, only Alchemist achieved this feat while leaving the ocean backdrop unedited. When the researchers trained comparable diffusion model InstructPix2Pix on the same data as their method for comparison, they found that Alchemist achieved superior accuracy scores. Likewise, a user study revealed that the MIT model was preferred and seen as more photorealistic than its counterpart.\\nKeeping it real with synthetic data\\nAccording to the researchers, collecting real data was impractical. Instead, they trained their model on a synthetic dataset, randomly editing the material attributes of 1,200 materials applied to 100 publicly available, unique 3D objects in Blender, a popular computer graphics design tool.‚ÄúThe control of generative AI image synthesis has so far been constrained by what text can describe,‚Äù says Fr√©do Durand, the Amar Bose Professor of Computing in the MIT Department of Electrical Engineering and Computer Science (EECS) and CSAIL member, who is a senior author on the paper. ‚ÄúThis work opens new and finer-grain control for visual attributes inherited from decades of computer-graphics research.‚Äù\"Alchemist is the kind of technique that\\'s needed to make machine learning and diffusion models practical and useful to the CGI community and graphic designers,‚Äù adds Google Research senior software engineer and co-author Mark Matthews. ‚ÄúWithout it, you\\'re stuck with this kind of uncontrollable stochasticity. It\\'s maybe fun for a while, but at some point, you need to get real work done and have it obey a creative vision.\"\\nSharma‚Äôs latest project comes a year after he led research on Materialistic, a machine-learning method that can identify similar materials in an image. This previous work demonstrated how AI models can refine their material understanding skills, and like Alchemist, was fine-tuned on a synthetic dataset of 3D models from Blender.\\nStill, Alchemist has a few limitations at the moment. The model struggles to correctly infer illumination, so it occasionally fails to follow a user‚Äôs input. Sharma notes that this method sometimes generates physically implausible transparencies, too. Picture a hand partially inside a cereal box, for example ‚Äî at Alchemist‚Äôs maximum setting for this attribute, you‚Äôd see a clear container without the fingers reaching in.The researchers would like to expand on how such a model could improve 3D assets for graphics at scene level. Also, Alchemist could help infer material properties from images. According to Sharma, this type of work could unlock links between objects\\' visual and mechanical traits in the future.\\nMIT EECS professor and CSAIL member William T. Freeman is also a senior author, joining Varun Jampani, and Google Research scientists Yuanzhen Li PhD ‚Äô09, Xuhui Jia, and Dmitry Lagun. The work was supported, in part, by a National Science Foundation grant and gifts from Google and Amazon. The group‚Äôs work will be highlighted at CVPR in June.\\nShare this news article on:\\nPaper\\nRelated Links\\nRelated Topics\\nRelated Articles\\nUsing AI to protect against AI image manipulation\\nResearchers use AI to identify similar materials in images\\nTechnique enables real-time rendering of scenes in 3D\\nUsing computers to view the unseen\\nPrevious item\\nNext item\\nMore MIT News\\nMaking climate models relevant for local decision-makers\\nRead full story ‚Üí\\nNew algorithm discovers language just by watching videos\\nRead full story ‚Üí\\nNew computer vision method helps speed up screening of electronic materials\\nRead full story ‚Üí\\nMIT Faculty Founder Initiative announces three winners of entrepreneurship awards\\nRead full story ‚Üí\\nHow a quantum scientist, a nurse, and an economist are joining the fight against global poverty\\nRead full story ‚Üí\\nCatalyst Symposium helps lower ‚Äúactivation barriers‚Äù for rising biology researchers\\nRead full story ‚Üí\\nMore about MIT News at Massachusetts Institute of Technology\\nThis website is managed by the MIT News Office, part of the Institute Office of Communications.\\nNews by Schools/College:\\nResources:\\nTools:\\nMassachusetts Institute of Technology77 Massachusetts Avenue, Cambridge, MA, USA', 'time': None}, {'url': 'https://news.mit.edu/2024/ai-generates-high-quality-images-30-times-faster-single-step-0321', 'raw_content': \"Suggestions or feedback?\\nMIT News | Massachusetts Institute of Technology\\nBrowse By\\nTopics\\nDepartments\\nCenters, Labs, & Programs\\nSchools\\nBreadcrumb\\nAI generates high-quality images 30 times faster in a single step\\nPress Contact:\\nPrevious image\\nNext image\\nIn our current age of artificial intelligence, computers can generate their own ‚Äúart‚Äù by way of diffusion models, iteratively adding structure to a noisy initial state until a clear image or video emerges. Diffusion models have suddenly grabbed a seat at everyone‚Äôs table: Enter a few words and experience instantaneous, dopamine-spiking dreamscapes at the intersection of reality and fantasy. Behind the scenes, it involves a complex, time-intensive process requiring numerous iterations for the algorithm to perfect the image.\\nMIT Computer Science and Artificial Intelligence Laboratory (CSAIL) researchers have introduced a new framework that simplifies the multi-step process of traditional diffusion models into a single step, addressing previous limitations. This is done through a type of teacher-student model: teaching a new computer model to mimic the behavior of more complicated, original models that generate images. The approach, known as distribution matching distillation (DMD), retains the quality of the generated images and allows for much faster generation.\\n‚ÄúOur work is a novel method that accelerates current diffusion models such as Stable Diffusion and DALLE-3 by 30 times,‚Äù says Tianwei Yin, an MIT PhD student in electrical engineering and computer science, CSAIL affiliate, and the lead researcher on the DMD framework. ‚ÄúThis advancement not only significantly reduces computational time but also retains, if not surpasses, the quality of the generated visual content. Theoretically, the approach marries the principles of generative adversarial networks (GANs) with those of diffusion models, achieving visual content generation in a single step ‚Äî a stark contrast to the hundred steps of iterative refinement required by current diffusion models. It could potentially be a new generative modeling method that excels in speed and quality.‚Äù\\nThis single-step diffusion model could enhance design tools, enabling quicker content creation and potentially supporting advancements in drug discovery and 3D modeling, where promptness and efficacy are key.\\nDistribution dreams\\nDMD cleverly has two components. First, it uses a regression loss, which anchors the mapping to ensure a coarse organization of the space of images to make training more stable. Next, it uses a distribution matching loss, which ensures that the probability to generate a given image with the student model corresponds to its real-world occurrence frequency. To do this, it leverages two diffusion models that act as guides, helping the system understand the difference between real and generated images and making training the speedy one-step generator possible.\\nThe system achieves faster generation by training a new network to minimize the distribution divergence between its generated images and those from the training dataset used by traditional diffusion models. ‚ÄúOur key insight is to approximate gradients that guide the improvement of the new model using two diffusion models,‚Äù says Yin. ‚ÄúIn this way, we distill the knowledge of the original, more complex model into the simpler, faster one, while bypassing the notorious instability and mode collapse issues in GANs.‚Äù\\nYin and colleagues used pre-trained networks for the new student model, simplifying the process. By copying and fine-tuning parameters from the original models, the team achieved fast training convergence of the new model, which is capable of producing high-quality images with the same architectural foundation. ‚ÄúThis enables combining with other system optimizations based on the original architecture to further accelerate the creation process,‚Äù adds Yin.\\nWhen put to the test against the usual methods, using a wide range of benchmarks, DMD showed consistent performance. On the popular benchmark of generating images based on specific classes on ImageNet, DMD is the first one-step diffusion technique that churns out pictures pretty much on par with those from the original, more complex models, rocking a super-close Fr√©chet inception distance (FID) score of just 0.3, which is impressive, since FID is all about judging the quality and diversity of generated images. Furthermore, DMD excels in industrial-scale text-to-image generation and achieves state-of-the-art one-step generation performance. There's still a slight quality gap when tackling trickier text-to-image applications, suggesting there's a bit of room for improvement down the line.\\nAdditionally, the performance of the DMD-generated images is intrinsically linked to the capabilities of the teacher model used during the distillation process. In the current form, which uses Stable Diffusion v1.5 as the teacher model, the student inherits limitations such as rendering detailed depictions of text and small faces, suggesting that DMD-generated images could be further enhanced by more advanced teacher models.\\n‚ÄúDecreasing the number of iterations has been the Holy Grail in diffusion models since their inception,‚Äù says Fredo Durand, MIT professor of electrical engineering and computer science, CSAIL principal investigator, and a lead author on the paper. ‚ÄúWe are very excited to finally enable single-step image generation, which will dramatically reduce compute costs and accelerate the process.‚Äù\\n‚ÄúFinally, a paper that successfully combines the versatility and high visual quality of diffusion models with the real-time performance of GANs,‚Äù says Alexei Efros, a professor of electrical engineering and computer science at the University of California at Berkeley who was not involved in this study. ‚ÄúI expect this work to open up fantastic possibilities for high-quality real-time visual editing.‚Äù\\nYin and Durand‚Äôs fellow authors are MIT electrical engineering and computer science professor and CSAIL principal investigator William T. Freeman, as well as Adobe research scientists Micha√´l Gharbi SM '15, PhD '18; Richard Zhang; Eli Shechtman; and Taesung Park. Their work was supported, in part, by U.S. National Science Foundation grants (including one for the Institute for Artificial Intelligence and Fundamental Interactions), the Singapore Defense Science and Technology Agency, and by funding from Gwangju Institute of Science and Technology and Amazon. Their work will be presented at the Conference on Computer Vision and Pattern Recognition in June.\\nShare this news article on:\\nPaper\\nRelated Links\\nRelated Topics\\nRelated Articles\\nNew algorithm unlocks high-resolution insights for computer vision\\nHelping computer vision and language models understand what they see\\nComputer vision system marries image recognition and generation\\nPrevious item\\nNext item\\nMore MIT News\\nMaking climate models relevant for local decision-makers\\nRead full story ‚Üí\\nNew algorithm discovers language just by watching videos\\nRead full story ‚Üí\\nNew computer vision method helps speed up screening of electronic materials\\nRead full story ‚Üí\\nMIT Faculty Founder Initiative announces three winners of entrepreneurship awards\\nRead full story ‚Üí\\nHow a quantum scientist, a nurse, and an economist are joining the fight against global poverty\\nRead full story ‚Üí\\nCatalyst Symposium helps lower ‚Äúactivation barriers‚Äù for rising biology researchers\\nRead full story ‚Üí\\nMore about MIT News at Massachusetts Institute of Technology\\nThis website is managed by the MIT News Office, part of the Institute Office of Communications.\\nNews by Schools/College:\\nResources:\\nTools:\\nMassachusetts Institute of Technology77 Massachusetts Avenue, Cambridge, MA, USA\", 'time': None}, {'url': 'https://scitechdaily.com/mits-new-generative-ai-outperforms-diffusion-models-in-image-generation/', 'raw_content': 'MIT‚Äôs New Generative AI Outperforms Diffusion Models in Image Generation\\nBy Rachel Gordon, MIT CSAIL October 14, 2023\\nMIT‚Äôs CSAIL introduces the PFGM++, an AI model combining diffusion and Poisson Flow principles. It offers superior image generation by replicating electric field behaviors, representing a leap in generative AI.\\nInspired by physics, a new generative model PFGM++ outperforms diffusion models in image generation.\\nGenerative AI, which is currently riding a crest of popular discourse, promises a world where the simple transforms into the complex ‚Äî where a simple distribution evolves into intricate patterns of images, sounds, or text, rendering the artificial startlingly real.\\nThe realms of imagination no longer remain as mere abstractions, as researchers from MIT‚Äôs Computer Science and Artificial Intelligence Laboratory (CSAIL) have brought an innovative AI model to life. Their new technology integrates two seemingly unrelated physical laws that underpin the best-performing generative models to date: diffusion, which typically illustrates the random motion of elements, like heat permeating a room or a gas expanding into space, and Poisson Flow, which draws on the principles governing the activity of electric charges.\\nA New Model Emerges\\nThis harmonious blend has resulted in superior performance in generating new images, outpacing existing state-of-the-art models. Since its inception, the ‚ÄúPoisson Flow Generative Model ++‚Äù (PFGM++) has found potential applications in various fields, from antibody and RNA sequence generation to audio production and graph generation.\\nThe model can generate complex patterns, like creating realistic images or mimicking real-world processes. PFGM++ builds off of PFGM, the team‚Äôs work from the prior year. PFGM takes inspiration from the means behind the mathematical equation known as the ‚ÄúPoisson‚Äù equation, and then applies it to the data the model tries to learn from. To do this, the team used a clever trick: They added an extra dimension to their model‚Äôs ‚Äúspace,‚Äù kind of like going from a 2D sketch to a 3D model. This extra dimension gives more room for maneuvering, places the data in a larger context, and helps one approach the data from all directions when generating new samples.\\n‚ÄúPFGM++ is an example of the kinds of AI advances that can be driven through interdisciplinary collaborations between physicists and computer scientists,‚Äù says Jesse Thaler, theoretical particle physicist in MIT‚Äôs Laboratory for Nuclear Science‚Äôs Center for Theoretical Physics and director of the National Science Foundation‚Äôs AI Institute for Artificial Intelligence and Fundamental Interactions (NSF AI IAIFI), who was not involved in the work.\\n‚ÄúIn recent years, AI-based generative models have yielded numerous eye-popping results, from photorealistic images to lucid streams of text. Remarkably, some of the most powerful generative models are grounded in time-tested concepts from physics, such as symmetries and thermodynamics. PFGM++ takes a century-old idea from fundamental physics ‚Äî that there might be extra dimensions of space-time ‚Äî and turns it into a powerful and robust tool to generate synthetic but realistic datasets. I‚Äôm thrilled to see the myriad of ways ‚Äòphysics intelligence‚Äô is transforming the field of artificial intelligence.‚Äù\\nUnderlying Mechanics\\nThe underlying mechanism of PFGM isn‚Äôt as complex as it might sound. The researchers compared the data points to tiny electric charges placed on a flat plane in a dimensionally expanded world. These charges produce an ‚Äúelectric field,‚Äù with the charges looking to move upwards along the field lines into an extra dimension and consequently forming a uniform distribution on a vast imaginary hemisphere. The generation process is like rewinding a videotape: starting with a uniformly distributed set of charges on the hemisphere and tracking their journey back to the flat plane along the electric lines, they align to match the original data distribution. This intriguing process allows the neural model to learn the electric field, and generate new data that mirrors the original.\\nThe PFGM++ model extends the electric field in PFGM to an intricate, higher-dimensional framework. When you keep expanding these dimensions, something unexpected happens ‚Äî the model starts resembling another important class of models, the diffusion models. This work is all about finding the right balance. The PFGM and diffusion models sit at opposite ends of a spectrum: one is robust but complex to handle, the other simpler but less sturdy. The PFGM++ model offers a sweet spot, striking a balance between robustness and ease of use. This innovation paves the way for more efficient image and pattern generation, marking a significant step forward in technology. Along with adjustable dimensions, the researchers proposed a new training method that enables more efficient learning of the electric field.\\nPutting Theory to the Test\\nTo bring this theory to life, the team resolved a pair of differential equations detailing these charges‚Äô motion within the electric field. They evaluated the performance using the Frechet Inception Distance (FID) score, a widely accepted metric that assesses the quality of images generated by the model in comparison to the real ones. PFGM++ further showcases a higher resistance to errors and robustness toward the step size in the differential equations.\\nLooking ahead, they aim to refine certain aspects of the model, particularly in systematic ways to identify the ‚Äúsweet spot‚Äù value of D tailored for specific data, architectures, and tasks by analyzing the behavior of estimation errors of neural networks. They also plan to apply the PFGM++ to the modern large-scale text-to-image/text-to-video generation.\\nIndustry Feedback\\n‚ÄúDiffusion models have become a critical driving force behind the revolution in generative AI,‚Äù says Yang Song, research scientist at OpenAI. ‚ÄúPFGM++ presents a powerful generalization of diffusion models, allowing users to generate higher-quality images by improving the robustness of image generation against perturbations and learning errors. Furthermore, PFGM++ uncovers a surprising connection between electrostatics and diffusion models, providing new theoretical insights into diffusion model research.‚Äù\\n‚ÄúPoisson Flow Generative Models do not only rely on an elegant physics-inspired formulation based on electrostatics, but they also offer state-of-the-art generative modeling performance in practice,‚Äù says NVIDIA Senior Research Scientist Karsten Kreis, who was not involved in the work.\\n‚ÄúThey even outperform the popular diffusion models, which currently dominate the literature. This makes them a very powerful generative modeling tool, and I envision their application in diverse areas, ranging from digital content creation to generative drug discovery. More generally, I believe that the exploration of further physics-inspired generative modeling frameworks holds great promise for the future and that Poisson Flow Generative Models are only the beginning.‚Äù\\nReference: ‚ÄúPFGM++: Unlocking the Potential of Physics-Inspired Generative Models‚Äù by Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark and Tommi Jaakkola, 10 February 2023, Computer Science > Machine Learning. arXiv:2302.04265\\nAuthors on a paper about this work include three MIT graduate students: Yilun Xu of the Department of Electrical Engineering and Computer Science (EECS) and CSAIL, Ziming Liu of the Department of Physics and the NSF AI IAIFI, and Shangyuan Tong of EECS and CSAIL, as well as Google Senior Research Scientist Yonglong Tian PhD ‚Äô23. MIT professors Max Tegmark and Tommi Jaakkola advised the research.\\nThe team was supported by the MIT-DSTA Singapore collaboration, the MIT-IBM Watson AI Lab, National Science Foundation grants, The Casey and Family Foundation, the Foundational Questions Institute, the Rothberg Family Fund for Cognitive Science, and the ML for Pharmaceutical Discovery and Synthesis Consortium. Their work was presented at the International Conference on Machine Learning this summer.\\nMore on SciTechDaily\\nTechnology\\nMIT‚Äôs Clever Way To Clean Solar Panels Without Water\\nScience\\nWorking Memory: How the Brain Focuses on What‚Äôs in Mind\\nSpace\\nISS Astronauts Conduct Earth Observations, Space Biology and Hardware Installs\\nHealth\\nThe Shopping Trip That Saves Your Life: Supermarket Carts That Diagnose Heart Problems\\nPhysics\\nLarge Hadron Collider: First Detection of Exotic ‚ÄúX‚Äù Particles in Quark-Gluon Plasma\\nHealth\\nUprooting Cancer: Innovative Hydrogel Rapidly Reverts Cancer Cells Back to Cancer Stem Cells\\nBiology\\nYale Researchers Track How Cells Repair Rips in DNA\\nEarth\\nGlobal Pollution Estimates Reveal Surprises, As Well as Opportunity\\nBe the first to comment on \"MIT‚Äôs New Generative AI Outperforms Diffusion Models in Image Generation\"\\nLeave a comment Cancel reply\\nEmail address is optional. If provided, your email will not be published or shared.\\nComment\\nName\\nEmail\\nSave my name, email, and website in this browser for the next time I comment.\\nSubscribe\\nSciTechDaily: Home of the best science and technology news since 1998. Keep up with the latest scitech news via email or social media. \\xa0 > Subscribe Free to Email Digest\\nPopular Articles\\nJune 10, 2024\\nSolving an Eons-Old Mystery: Paleontologists Shed New Light on the Extinction of the Wooly Rhinoceros\\nAdvanced computer modeling reveals that sustained human hunting contributed to the woolly rhinoceros‚Äôs extinction by blocking their migration to new habitats during post-Ice Age warming,‚Ä¶\\nJune 10, 2024\\nInterstellar Intruder: The Cosmic Event That Rewrote Earth‚Äôs Climate History\\nJune 9, 2024\\nMayo Clinic Study Reveals Startling Connection Between Energy Drinks and Sudden Cardiac Arrest\\nJune 9, 2024\\nShaking Up Seismology: Geometry as the Groundbreaking Predictor of Earthquakes\\nJune 9, 2024\\nJohns Hopkins Scientists Discover Unusual New Hero in Evolution\\nJune 9, 2024\\nCommon Food Additive Found in Ice Cream, Chocolate, and Bread Linked to Diabetes\\nJune 9, 2024\\nBeyond Einstein: Groundbreaking Map of the Universe Redefines Cosmic Models\\nJune 9, 2024\\nThe Megafauna Mystery: Scientists Discover New Clues to What Happened to North America‚Äôs Largest Animals\\nTags\\nFollow SciTechDaily\\nSciTech News\\nLatest News\\nCopyright ¬© 1998 - 2024 SciTechDaily. All Rights Reserved.', 'time': None}, {'url': 'https://github.com/cvlab-stonybrook/Large-Image-Diffusion', 'raw_content': 'Navigation Menu\\nSearch code, repositories, users, issues, pull requests...\\nProvide feedback\\nWe read every piece of feedback, and take your input very seriously.\\nSaved searches\\nUse saved searches to filter your results more quickly\\nTo see all available qualifiers, see our documentation.\\nCVPR 2024: Learned representation-guided diffusion models for large-image generation\\ncvlab-stonybrook/Large-Image-Diffusion\\nFolders and files\\nLatest commit\\nHistory\\nconfigs/latent-diffusion\\nconfigs/latent-diffusion\\nldm\\nldm\\nnotebooks\\nnotebooks\\n.gitignore\\n.gitignore\\nREADME.md\\nREADME.md\\nenvironment.yaml\\nenvironment.yaml\\nmain.py\\nmain.py\\nmydiff.py\\nmydiff.py\\nsetup.py\\nsetup.py\\nteaser.png\\nteaser.png\\nRepository files navigation\\nLearned representation-guided diffusion models for large-image generation\\nOfficial code for our CVPR 2024 publication Learned representation-guided diffusion models for large-image generation. This codebase builds heavily on CompVis/latent-diffusion and PathLDM.\\nRequirements\\nTo install python dependencies,\\nDownloading + Organizing Data\\nDue to storage limitations, we cannot upload the image patches and embeddings used for training. However, training data can be curated by following these steps -\\nDownload the WSIs\\nWe train diffusion models on TCGA-BRCA, TCGA-CRC and Chesapeake Land cover datasets. For BRCA and CRC, we used the DSMIL repository to extract 256 x 256 patches @ 20x magnification, and conditioned the diffusion models from HIPT and iBOT. We also train a model on 5x BRCA patches, conditioned on CTransPath embeddings See Section 4.1 from our paper for more details.\\nPrepare the patches\\nOnce you clone the DSMIL repository, you can use the following command to extract patches from the WSIs.\\nSSL embeddings\\nFollow instructions in HIPT / iBOT repository to extract embeddings for each patch.\\nPretrained models\\nWe provide the following trained models\\nTraining\\nExample training command:\\nSampling\\nRefer to these notebooks for generating images using the provided models:\\nBibtex\\nAbout\\nCVPR 2024: Learned representation-guided diffusion models for large-image generation\\nResources\\nStars\\nWatchers\\nForks\\nReleases\\nPackages\\n0\\nLanguages\\nFooter\\nFooter navigation', 'time': None}, {'url': 'https://arxiv.org/abs/2406.02347', 'raw_content': 'In this paper, we propose an efficient, fast, and versatile distillation\\nmethod to accelerate the generation of pre-trained diffusion models: Flash\\nDiffusion. The method reaches state-of-the-art performances in terms of FID and\\nCLIP-Score for few steps image generation on the COCO2014 and COCO2017\\ndatasets, while requiring only several GPU hours of training and fewer\\ntrainable parameters than existing methods. In addition to its efficiency, the\\nversatility of the method is also exposed across several tasks such as\\ntext-to-image, inpainting, face-swapping, super-resolution and using different\\nbackbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\\\\alpha$),\\nas well as adapters. In all cases, the method allowed to reduce drastically the\\nnumber of sampling steps while maintaining very high-quality image generation.\\nThe official implementation is available at\\nhttps://github.com/gojasper/flash-diffusion.', 'time': datetime.date(2024, 6, 5)}])\n",
            "üìÉ Source: https://news.mit.edu/2024/ai-generates-high-quality-images-30-times-faster-single-step-0321\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: ‚ÄúOur work is a novel method that accelerates current diffusion models such as Stable Diffusion and DALLE-3 by 30 times,‚Äù says Tianwei Yin, an MIT PhD student in electrical engineering and computer science, CSAIL affiliate, and the lead researcher on the DMD framework. ‚ÄúThis advancement not only significantly reduces computational time but also retains, if not surpasses, the quality of the generated visual content. Theoretically, the approach marries the principles of generative adversarial networks (GANs) with those of diffusion models, achieving visual content generation in a single step ‚Äî a stark contrast to the hundred steps of iterative refinement required by current diffusion models. It could potentially be a new generative modeling method that excels in speed and quality.‚Äù\n",
            "This single-step diffusion model could enhance design tools, enabling quicker content creation and potentially supporting advancements in drug discovery and 3D modeling, where promptness and efficacy are key.\n",
            "\n",
            "Source: https://news.mit.edu/2024/ai-generates-high-quality-images-30-times-faster-single-step-0321\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: Additionally, the performance of the DMD-generated images is intrinsically linked to the capabilities of the teacher model used during the distillation process. In the current form, which uses Stable Diffusion v1.5 as the teacher model, the student inherits limitations such as rendering detailed depictions of text and small faces, suggesting that DMD-generated images could be further enhanced by more advanced teacher models.\n",
            "‚ÄúDecreasing the number of iterations has been the Holy Grail in diffusion models since their inception,‚Äù says Fredo Durand, MIT professor of electrical engineering and computer science, CSAIL principal investigator, and a lead author on the paper. ‚ÄúWe are very excited to finally enable single-step image generation, which will dramatically reduce compute costs and accelerate the process.‚Äù\n",
            "\n",
            "Source: https://arxiv.org/abs/2406.02347\n",
            "Title: \n",
            "Published_time: 2024-06-05\n",
            "Content: In this paper, we propose an efficient, fast, and versatile distillation\n",
            "method to accelerate the generation of pre-trained diffusion models: Flash\n",
            "Diffusion. The method reaches state-of-the-art performances in terms of FID and\n",
            "CLIP-Score for few steps image generation on the COCO2014 and COCO2017\n",
            "datasets, while requiring only several GPU hours of training and fewer\n",
            "trainable parameters than existing methods. In addition to its efficiency, the\n",
            "versatility of the method is also exposed across several tasks such as\n",
            "text-to-image, inpainting, face-swapping, super-resolution and using different\n",
            "backbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\\alpha$),\n",
            "as well as adapters. In all cases, the method allowed to reduce drastically the\n",
            "number of sampling steps while maintaining very high-quality image generation.\n",
            "The official implementation is available at\n",
            "https://github.com/gojasper/flash-diffusion.\n",
            "\n",
            "Source: https://scitechdaily.com/mits-new-generative-ai-outperforms-diffusion-models-in-image-generation/\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: MIT‚Äôs New Generative AI Outperforms Diffusion Models in Image Generation\n",
            "By Rachel Gordon, MIT CSAIL October 14, 2023\n",
            "MIT‚Äôs CSAIL introduces the PFGM++, an AI model combining diffusion and Poisson Flow principles. It offers superior image generation by replicating electric field behaviors, representing a leap in generative AI.\n",
            "Inspired by physics, a new generative model PFGM++ outperforms diffusion models in image generation.\n",
            "Generative AI, which is currently riding a crest of popular discourse, promises a world where the simple transforms into the complex ‚Äî where a simple distribution evolves into intricate patterns of images, sounds, or text, rendering the artificial startlingly real.\n",
            "\n",
            "Source: https://scitechdaily.com/mits-new-generative-ai-outperforms-diffusion-models-in-image-generation/\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: Industry Feedback\n",
            "‚ÄúDiffusion models have become a critical driving force behind the revolution in generative AI,‚Äù says Yang Song, research scientist at OpenAI. ‚ÄúPFGM++ presents a powerful generalization of diffusion models, allowing users to generate higher-quality images by improving the robustness of image generation against perturbations and learning errors. Furthermore, PFGM++ uncovers a surprising connection between electrostatics and diffusion models, providing new theoretical insights into diffusion model research.‚Äù\n",
            "‚ÄúPoisson Flow Generative Models do not only rely on an elegant physics-inspired formulation based on electrostatics, but they also offer state-of-the-art generative modeling performance in practice,‚Äù says NVIDIA Senior Research Scientist Karsten Kreis, who was not involved in the work.\n",
            "\n",
            "Source: https://news.mit.edu/2024/ai-generates-high-quality-images-30-times-faster-single-step-0321\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: When put to the test against the usual methods, using a wide range of benchmarks, DMD showed consistent performance. On the popular benchmark of generating images based on specific classes on ImageNet, DMD is the first one-step diffusion technique that churns out pictures pretty much on par with those from the original, more complex models, rocking a super-close Fr√©chet inception distance (FID) score of just 0.3, which is impressive, since FID is all about judging the quality and diversity of generated images. Furthermore, DMD excels in industrial-scale text-to-image generation and achieves state-of-the-art one-step generation performance. There's still a slight quality gap when tackling trickier text-to-image applications, suggesting there's a bit of room for improvement down the line.\n",
            "\n",
            "Source: https://news.mit.edu/2024/ai-generates-high-quality-images-30-times-faster-single-step-0321\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: Suggestions or feedback?\n",
            "MIT News | Massachusetts Institute of Technology\n",
            "Browse By\n",
            "Topics\n",
            "Departments\n",
            "Centers, Labs, & Programs\n",
            "Schools\n",
            "Breadcrumb\n",
            "AI generates high-quality images 30 times faster in a single step\n",
            "Press Contact:\n",
            "Previous image\n",
            "Next image\n",
            "In our current age of artificial intelligence, computers can generate their own ‚Äúart‚Äù by way of diffusion models, iteratively adding structure to a noisy initial state until a clear image or video emerges. Diffusion models have suddenly grabbed a seat at everyone‚Äôs table: Enter a few words and experience instantaneous, dopamine-spiking dreamscapes at the intersection of reality and fantasy. Behind the scenes, it involves a complex, time-intensive process requiring numerous iterations for the algorithm to perfect the image.\n",
            "\n",
            "Source: https://news.mit.edu/2024/ai-generates-high-quality-images-30-times-faster-single-step-0321\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: ‚ÄúFinally, a paper that successfully combines the versatility and high visual quality of diffusion models with the real-time performance of GANs,‚Äù says Alexei Efros, a professor of electrical engineering and computer science at the University of California at Berkeley who was not involved in this study. ‚ÄúI expect this work to open up fantastic possibilities for high-quality real-time visual editing.‚Äù\n",
            "\n",
            "\n",
            "üîé Running research for 'recent publications on image generation using diffusion models June 2024'...\n",
            "‚úÖ Added source url to research: https://arxiv.org/pdf/2403.06807\n",
            "\n",
            "‚úÖ Added source url to research: https://www.amazon.science/publications/on-the-scalability-of-diffusion-based-text-to-image-generation\n",
            "\n",
            "‚úÖ Added source url to research: https://arxiv.org/pdf/2406.03184\n",
            "\n",
            "‚úÖ Added source url to research: https://community.openai.com/t/seeking-ideas-for-text-to-image-generation/806633\n",
            "\n",
            "ü§î Researching for relevant information...\n",
            "\n",
            "arxiv\n",
            "bs\n",
            "arxiv\n",
            "bs\n",
            "üìù Getting relevant content based on query: recent publications on image generation using diffusion models June 2024...\n",
            "base_compressor=DocumentCompressorPipeline(transformers=[<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x000001865CD9FF90>, EmbeddingsFilter(embeddings=OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001865AA0E090>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001865AA301D0>, model='text-embedding-3-small', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True), similarity_fn=<function cosine_similarity at 0x00000186583FFE20>, k=20, similarity_threshold=0.2)]) base_retriever=SearchAPIRetriever(pages=[{'url': 'https://arxiv.org/pdf/2403.06807', 'raw_content': 'Diffusion models are relatively easy to train but require many steps to\\ngenerate samples. Consistency models are far more difficult to train, but\\ngenerate samples in a single step.\\n  In this paper we propose Multistep Consistency Models: A unification between\\nConsistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that\\ncan interpolate between a consistency model and a diffusion model: a trade-off\\nbetween sampling speed and sampling quality. Specifically, a 1-step consistency\\nmodel is a conventional consistency model whereas a $\\\\infty$-step consistency\\nmodel is a diffusion model.\\n  Multistep Consistency Models work really well in practice. By increasing the\\nsample budget from a single step to 2-8 steps, we can train models more easily\\nthat generate higher quality samples, while retaining much of the sampling\\nspeed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1\\nFID on Imagenet128 in 8 steps with consistency distillation, using simple\\nlosses without adversarial training. We also show that our method scales to a\\ntext-to-image diffusion model, generating samples that are close to the quality\\nof the original model.', 'time': datetime.date(2024, 6, 3)}, {'url': 'https://arxiv.org/pdf/2406.03184', 'raw_content': \"Existing single image-to-3D creation methods typically involve a two-stage\\nprocess, first generating multi-view images, and then using these images for 3D\\nreconstruction. However, training these two stages separately leads to\\nsignificant data bias in the inference phase, thus affecting the quality of\\nreconstructed results. We introduce a unified 3D generation framework, named\\nOuroboros3D, which integrates diffusion-based multi-view image generation and\\n3D reconstruction into a recursive diffusion process. In our framework, these\\ntwo modules are jointly trained through a self-conditioning mechanism, allowing\\nthem to adapt to each other's characteristics for robust inference. During the\\nmulti-view denoising process, the multi-view diffusion model uses the 3D-aware\\nmaps rendered by the reconstruction module at the previous timestep as\\nadditional conditions. The recursive diffusion framework with 3D-aware feedback\\nunites the entire process and improves geometric consistency.Experiments show\\nthat our framework outperforms separation of these two stages and existing\\nmethods that combine them at the inference phase. Project page:\\nhttps://costwen.github.io/Ouroboros3D/\", 'time': datetime.date(2024, 6, 5)}, {'url': 'https://community.openai.com/t/seeking-ideas-for-text-to-image-generation/806633', 'raw_content': 'Seeking ideas for text to image generation\\nHello everyone,\\nI am a university student currently working on my final year research project, and I am looking for ideas related to text-to-image generation. My goal is to identify a meaningful research gap in this field that I can explore.\\nSo far, I have reviewed several existing models and techniques, such as DALL-E and other GAN-based methods, but I am struggling to pinpoint a specific area that has not been extensively covered. I am particularly interested in topics that could contribute to improving image quality, semantic coherence, or other aspects of text-to-image models.\\nCould anyone recommend some potential research areas or gaps in the current literature that I could investigate? Any suggestions, papers, or resources would be greatly appreciated.\\nThank you in advance for your help!\\nThere are two kinds of image models: GAN and Diffusion. Currently, most people are using diffusion for their models, such as DALL-E and Stable Diffusion, as they are the best. An idea would be to do more research on GAN, as that has significantly slowed down. Another idea would be to create image upscalers using GANs, as I don‚Äôt believe I have ever seen a GAN upscaler before, as they are mostly diffusion models. I made a GAN a bit ago and some code and results can be found here: GitHub - grandell1234/S.C.O.R.P: Text-To-Image GAN Model\\nDoes creating image upscalers using GANs, means inputting a low quality image and generating a high quality image from that. like the Super-Resolution Generative Adversarial Networks (SRGANs). Can you explain\\na bit more about it.\\nYeah, or taking it from like 512x512 to 1280x1280 adding pixels where required.\\nCurrently I ran into a problem where Dalle can‚Äôt divide the rendered image into exact spaces, for example a rectangle into 8 exact boxes, then making an image for every box. The problem is I‚Äôve seen it accomplished by one GPT. The other obvious issue is rendering text or counting. I think Dalle should add a layer of text object on top of the Dalle image then merge them prior to output. If the Coordinates can be matched then we resolve this issue completely by merging text logic with the image logic, then merging them prior to output. So an instruction to create art at X, Y coordinates of an image. Once this is mastered, commands can place text.\\nEven HTML can accomplish this simple concept of a background image under text. Dalle needs to understand its canvas space. To take a command like:\\nCreate 8 boxes with random art numbered 1-8, center text.\\nWe can‚Äôt even get Dalle to return 8 exact sized boxes of art yet.\\n‚Ä¶\\nRelated Topics\\nPowered by Discourse, best viewed with JavaScript enabled', 'time': None}])\n",
            "üìÉ Source: https://arxiv.org/pdf/2403.06807\n",
            "Title: \n",
            "Published_time: 2024-06-03\n",
            "Content: FID on Imagenet128 in 8 steps with consistency distillation, using simple\n",
            "losses without adversarial training. We also show that our method scales to a\n",
            "text-to-image diffusion model, generating samples that are close to the quality\n",
            "of the original model.\n",
            "\n",
            "Source: https://community.openai.com/t/seeking-ideas-for-text-to-image-generation/806633\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: Thank you in advance for your help!\n",
            "There are two kinds of image models: GAN and Diffusion. Currently, most people are using diffusion for their models, such as DALL-E and Stable Diffusion, as they are the best. An idea would be to do more research on GAN, as that has significantly slowed down. Another idea would be to create image upscalers using GANs, as I don‚Äôt believe I have ever seen a GAN upscaler before, as they are mostly diffusion models. I made a GAN a bit ago and some code and results can be found here: GitHub - grandell1234/S.C.O.R.P: Text-To-Image GAN Model\n",
            "Does creating image upscalers using GANs, means inputting a low quality image and generating a high quality image from that. like the Super-Resolution Generative Adversarial Networks (SRGANs). Can you explain\n",
            "a bit more about it.\n",
            "Yeah, or taking it from like 512x512 to 1280x1280 adding pixels where required.\n",
            "\n",
            "Source: https://arxiv.org/pdf/2406.03184\n",
            "Title: \n",
            "Published_time: 2024-06-05\n",
            "Content: Existing single image-to-3D creation methods typically involve a two-stage\n",
            "process, first generating multi-view images, and then using these images for 3D\n",
            "reconstruction. However, training these two stages separately leads to\n",
            "significant data bias in the inference phase, thus affecting the quality of\n",
            "reconstructed results. We introduce a unified 3D generation framework, named\n",
            "Ouroboros3D, which integrates diffusion-based multi-view image generation and\n",
            "3D reconstruction into a recursive diffusion process. In our framework, these\n",
            "two modules are jointly trained through a self-conditioning mechanism, allowing\n",
            "them to adapt to each other's characteristics for robust inference. During the\n",
            "multi-view denoising process, the multi-view diffusion model uses the 3D-aware\n",
            "maps rendered by the reconstruction module at the previous timestep as\n",
            "additional conditions. The recursive diffusion framework with 3D-aware feedback\n",
            "\n",
            "Source: https://community.openai.com/t/seeking-ideas-for-text-to-image-generation/806633\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: Seeking ideas for text to image generation\n",
            "Hello everyone,\n",
            "I am a university student currently working on my final year research project, and I am looking for ideas related to text-to-image generation. My goal is to identify a meaningful research gap in this field that I can explore.\n",
            "So far, I have reviewed several existing models and techniques, such as DALL-E and other GAN-based methods, but I am struggling to pinpoint a specific area that has not been extensively covered. I am particularly interested in topics that could contribute to improving image quality, semantic coherence, or other aspects of text-to-image models.\n",
            "Could anyone recommend some potential research areas or gaps in the current literature that I could investigate? Any suggestions, papers, or resources would be greatly appreciated.\n",
            "Thank you in advance for your help!\n",
            "\n",
            "Source: https://arxiv.org/pdf/2406.03184\n",
            "Title: \n",
            "Published_time: 2024-06-05\n",
            "Content: additional conditions. The recursive diffusion framework with 3D-aware feedback\n",
            "unites the entire process and improves geometric consistency.Experiments show\n",
            "that our framework outperforms separation of these two stages and existing\n",
            "methods that combine them at the inference phase. Project page:\n",
            "https://costwen.github.io/Ouroboros3D/\n",
            "\n",
            "Source: https://arxiv.org/pdf/2403.06807\n",
            "Title: \n",
            "Published_time: 2024-06-03\n",
            "Content: Diffusion models are relatively easy to train but require many steps to\n",
            "generate samples. Consistency models are far more difficult to train, but\n",
            "generate samples in a single step.\n",
            "  In this paper we propose Multistep Consistency Models: A unification between\n",
            "Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that\n",
            "can interpolate between a consistency model and a diffusion model: a trade-off\n",
            "between sampling speed and sampling quality. Specifically, a 1-step consistency\n",
            "model is a conventional consistency model whereas a $\\infty$-step consistency\n",
            "model is a diffusion model.\n",
            "  Multistep Consistency Models work really well in practice. By increasing the\n",
            "sample budget from a single step to 2-8 steps, we can train models more easily\n",
            "that generate higher quality samples, while retaining much of the sampling\n",
            "speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1\n",
            "FID on Imagenet128 in 8 steps with consistency distillation, using simple\n",
            "\n",
            "Source: https://community.openai.com/t/seeking-ideas-for-text-to-image-generation/806633\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: Yeah, or taking it from like 512x512 to 1280x1280 adding pixels where required.\n",
            "Currently I ran into a problem where Dalle can‚Äôt divide the rendered image into exact spaces, for example a rectangle into 8 exact boxes, then making an image for every box. The problem is I‚Äôve seen it accomplished by one GPT. The other obvious issue is rendering text or counting. I think Dalle should add a layer of text object on top of the Dalle image then merge them prior to output. If the Coordinates can be matched then we resolve this issue completely by merging text logic with the image logic, then merging them prior to output. So an instruction to create art at X, Y coordinates of an image. Once this is mastered, commands can place text.\n",
            "Even HTML can accomplish this simple concept of a background image under text. Dalle needs to understand its canvas space. To take a command like:\n",
            "Create 8 boxes with random art numbered 1-8, center text.\n",
            "\n",
            "Source: https://community.openai.com/t/seeking-ideas-for-text-to-image-generation/806633\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: Create 8 boxes with random art numbered 1-8, center text.\n",
            "We can‚Äôt even get Dalle to return 8 exact sized boxes of art yet.\n",
            "‚Ä¶\n",
            "Related Topics\n",
            "Powered by Discourse, best viewed with JavaScript enabled\n",
            "\n",
            "\n",
            "üîé Running research for ' What new research work was published in the past 30 days on image generation especially on disfussion based model?'...\n",
            "‚ùå URL already visited: https://arxiv.org/abs/2308.13142\n",
            "\n",
            "‚úÖ Added source url to research: https://www.nature.com/articles/s41467-024-49125-z\n",
            "\n",
            "‚ùå URL already visited: https://arxiv.org/pdf/2308.13142v1\n",
            "\n",
            "‚úÖ Added source url to research: https://arxiv.org/pdf/2209.00796v9\n",
            "\n",
            "‚úÖ Added source url to research: https://ieeexplore.ieee.org/document/10505362\n",
            "\n",
            "ü§î Researching for relevant information...\n",
            "\n",
            "bs\n",
            "arxiv\n",
            "bs\n",
            "üìù Getting relevant content based on query:  What new research work was published in the past 30 days on image generation especially on disfussion based model?...\n",
            "base_compressor=DocumentCompressorPipeline(transformers=[<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x000001865EBEA250>, EmbeddingsFilter(embeddings=OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001865AA0E090>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001865AA301D0>, model='text-embedding-3-small', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True), similarity_fn=<function cosine_similarity at 0x00000186583FFE20>, k=20, similarity_threshold=0.2)]) base_retriever=SearchAPIRetriever(pages=[{'url': 'https://www.nature.com/articles/s41467-024-49125-z', 'raw_content': 'Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\\nand JavaScript.\\nAdvertisement\\nDiffusion-based deep learning method for augmenting ultrastructural imaging and volume electron microscopy\\nNature Communications\\nvolume\\xa015, Article\\xa0number:\\xa04677 (2024)\\nCite this article\\n1565 Accesses\\n2 Altmetric\\nMetrics details\\nSubjects\\nAbstract\\nElectron microscopy (EM) revolutionized the way to visualize cellular ultrastructure. Volume EM (vEM) has further broadened its three-dimensional nanoscale imaging capacity. However, intrinsic trade-offs between imaging speed and quality of EM restrict the attainable imaging area and volume. Isotropic imaging with vEM for large biological volumes remains unachievable. Here, we developed EMDiffuse, a suite of algorithms designed to enhance EM and vEM capabilities, leveraging the cutting-edge image generation diffusion model. EMDiffuse generates realistic predictions with high resolution ultrastructural details and exhibits robust transferability by taking only one pair of images of 3 megapixels to fine-tune in denoising and super-resolution tasks. EMDiffuse also demonstrated proficiency in the isotropic vEM reconstruction task, generating isotropic volume even in the absence of isotropic training data. We demonstrated the robustness of EMDiffuse by generating isotropic volumes from seven public datasets obtained from different vEM techniques and instruments. The generated isotropic volume enables accurate three-dimensional nanoscale ultrastructure analysis. EMDiffuse also features self-assessment functionalities on predictions‚Äô reliability. We envision EMDiffuse to pave the way for investigations of the intricate subcellular nanoscale ultrastructure within large volumes of biological systems.\\nSimilar content being viewed by others\\nModular segmentation, spatial analysis and visualization of volume electron microscopy datasets\\nWhole-cell organelle segmentation in volume electron microscopy\\nDeep self-learning enables fast, high-fidelity isotropic resolution restoration for volumetric fluorescence microscopy\\nIntroduction\\nElectron microscopy (EM) is an essential tool for obtaining high-resolution images of biological specimens and has made a tremendous impact on cell biology, revealing highly complex cellular structures at the nanometer scale. Over the decades, EM has catalyzed numerous breakthroughs in life sciences, such as discovering novel cellular organelles, elucidating membrane structures, and generating detailed visualization of the macromolecular complexes1,2,3. Volume electron microscopy (vEM), a more recent innovation, arose from overcoming limitations of conventional two-dimensional electron microscopy techniques in studying 3D cellular structures4,5,6,7. vEM techniques such as serial section-based tomography techniques with transmission electron microscopy (TEM) and scanning electron microscopy (SEM), serial block-face SEM (SBF-SEM), and focused ion beam SEM (FIB-SEM) have facilitated investigations of 3D structures in cells, tissues and even small model organisms at nanometer resolution. Particularly, recent efforts in mapping brain connectomics with serial section-based vEM techniques8,9,10,11,12 and the developments of enhanced FIB-SEM for the acquisition of isotropic vEM data with improved robustness and throughput13,14,15 have further reinvigorated the field of EM and its applications.\\nHowever, the potential of EM and vEM is hindered by intrinsic limitations. It is essential to achieve a sufficient signal-to-noise ratio (SNR) to accurately map nanoscale cellular structures, which will inevitably reduce the imaging speed and prolong the dwell time per pixel, restricting the area and volume of imaging. Robust isotropic vEM also requires advanced instrumentation, such as enhanced FIB-SEM, which is only available at certain institutions or specialized centers. More importantly, the dogma of vEM is no viable method for generating isotropic datasets from large volumes. Currently, vEM is either limited to producing isotropic data from volumes of up to approximately 300\\u2009Œºm\\u2009√ó\\u2009300\\u2009Œºm\\u2009√ó\\u2009300\\u2009Œºm16, using state-of-the-art enhanced FIB-SEM, or capturing larger volumes aiming for cubic millimeter scale through serial section-based techniques10,17, which, however, suffers from unsatisfactory axial resolution due to the limited section thickness of diamond knife cuts.\\nIn recent years, computational methods and deep learning techniques have been applied to address the technical limitations of microscopy methods to expedite imaging processes and enhance the image quality of both light microscopy18,19,20,21 and electron microscopy22,23,24. Significant advances have been achieved in denoising and super-resolution tasks with deep learning techniques23,25,26,27,28. However, three major limitations still exist in the current methods. Firstly, most methods20,21,23,26,27,28,29 use a regression-based deep learning model, which emphasizes the learning of low-frequency mode30. Additionally, these frameworks employ L1 or L2 training objectives to train the model towards approximating the minimum mean square error (MMSE) of possible true prediction distribution, i.e., the average/median of all possible data interpretations of the learned posterior. This results in excessively smooth predictions with reduced resolution ‚Äì critical in microscopy ‚Äì defined as the smallest discernible distance between two distinct points on a specimen. A few methods utilize CycleGAN-based approaches, which may be susceptible to unstable training processes and limited capability in handling geometric transformations31. Secondly, many current methods are designed for light microscopy21,25,26,27, whereas EM images contain complex nanoscale structures, underexplored noise models, and single-channel limited image information, rendering image quality enhancement tasks more difficult. Third, few of the current methods can provide reliability assessments for their predictions. Since denoising and super-resolution are inherently ill-posed problems with multiple possible solutions, biologists may doubt the validity of the processed images without any uncertainty measures. CARE assesses aleatoric uncertainty by training networks to predict the mean and variance and assesses epistemic uncertainty by evaluating the agreement across five distinct networks, which introduce a significant increase in training overhead.\\nIn this study, we propose EMDiffuse, a diffusion model-based package for EM applications, aiming to enhance EM ultrastructural imaging and expand the realm of vEM capabilities. Diffusion models have demonstrated superiority over regression-based models32,33,34 and exhibit greater stability in training than GAN-based models35 regarding image generation and restoration tasks due to their distinctive diffusion-based training and inference schemes. Diffusion models also generate outputs with high-resolution details, which is critical for imaging intricate nanoscale cellular structures with EM. Here, we adopted the diffusion model for EM applications and developed EMDiffuse-n for EM denoising, EMDiffuse-r for EM super-resolution, and vEMDiffuse-i and vEMDiffuse-a for generating isotropic resolution data from anisotropic volumes for vEM. Moreover, we demonstrate the self-assessment capability of EMDiffuse, taking advantage of the inherent ability of diffusion models to generate an arbitrary number of diverse samples. The code is available in https://github.com/Luchixiang/EMDiffuse/36.\\nResults\\nEMDiffuse exhibited outstanding denoising performance\\nWe developed EMDiffuse-n, the noise reduction pipeline of EMDiffuse, to denoise EM ultrastructural images and accelerate EM imaging (Fig.\\xa01a). The EMDiffuse-n comprises three stages, which are data collection, image processing, and diffusion model (Fig.\\xa01a, ‚ÄúMethods‚Äù section). The first stage involved obtaining EM training pairs with different acquisition times (Supplementary Fig.\\xa01a, ‚ÄúMethods‚Äù section). Then, a hierarchical approach was employed to precisely align and register the noisy images with the reference image (Supplementary Fig.\\xa01b, ‚ÄúMethods‚Äù section). Finally, a diffusion model, namely UDiM (Supplementary Fig.\\xa02, ‚ÄúMethods‚Äù section), was trained to diminish the noise in well-aligned raw images. We observed extremely noisy training patterns lead to large gradients and subsequently cause drastic and undesired changes in model weights as well as instabilities during model training. To address this issue, we employed the difficulty-aware loss function (Supplementary Table\\xa01, ‚ÄúMethods‚Äù section)37, which downweights the contributions of such cases through a learned difficulty map, effectively mitigating the impact of these instances.\\na Schematic of EMDiffuse-n. In the training stage, paired data of various noise levels were collected and processed with a difficulty-aware training objective implemented. In the inference stage, EMDiffuse-n estimates and reduces the noise of raw EM images. Each test time, given an input image, EMDiffuse-n samples one potential output from the learned solution distribution. The average of the various outputs represents the final prediction, while the variance indicates the level of uncertainty. The uncertainty reflects the reliability of the prediction. b EMDiffuse reduces the noise and enhances the resolution of EM images with a representative region of a mouse brain cortex tissue with a pixel size of 3.3\\u2009nm processed by EMDiffuse-n, alongside a comparison of the resolution and artifacts in the same image processed with denoising algorithms. GT: ground truth image; Input: input image, CARE, PSSR, and RCAN: generated images by CARE, PSSR, and RCAN algorithms from the input image, respectively. Top right of each panel is the Fourier power spectrum. The resolution of processed images is shown on the Fourier transform image. Bottom panels are line profiles across the lines drawn in the Input panel. Min-max normalization was employed to scale the intensity values within each line plot. c Quantitative performance assessment of images processed with EMDiffuse, CARE, PSSR, and RCAN algorithms using the Learned Perceptual Image Patch Similarity (LPIPS, lower values indicate superior performance) and the resolution ratio (ground truth resolution/prediction resolution, higher values indicate higher image resolution). Each point represents a unique test image of the mouse brain cortex, n\\u2009=\\u2009960. d Uncertainty map for EMDiffuse processed images at different noise levels enables self-assessment of uncertainty values of the prediction. Predictions with uncertainty values of the image below 0.12 (‚ÄúMethods‚Äù section) are counted as reliable outputs. e EMDiffuse allows robust transferability. Representative input, EMDiffuse output without fine-tuning (w/o fine-tune), EMDiffuse fine-tuning with one training image (fine-tune), and GT images from the mouse liver, heart, bone marrow tissues with a pixel size of 4.4\\u2009nm and cultured HeLa cell sample with a pixel size of 2.1\\u2009nm. A magnified region is positioned in the top right corner. The resolution of each panel is shown at the bottom left corner with nm as the unit. The uncertainty value of the results is shown in the bottom left corner. U uncertainty value. Scale bars, a, d, e 0.3\\u2009Œºm; b 0.1\\u2009Œºm. Source data are provided as a Source Data file.\\nIn the inference stage, given an input image, UDiM samples one plausible solution from the learned solution distribution at each test time. Thus, it can generate an unlimited number of plausible outputs. Multiple outputs can be sampled and ensembled to improve the predictions of EMDiffuse-n or analyzed individually since recovering the noise-free images is an ill-posed problem (Fig.\\xa01a, Supplementary Fig.\\xa03a, b, ‚ÄúMethods‚Äù section). Specifically, we optimized the prediction generation methods based on the assessment of FSIM38 (Supplementary Fig.\\xa03c) and prediction resolution (Supplementary Fig.\\xa03d). We used K outputs (K\\u2009=\\u20092 in our experiments) for each raw input and computed the mean of these outputs, samples from the learned distribution, as the ultimate prediction (‚ÄúMethods‚Äù section, Supplementary Fig.\\xa03). Of note, we assessed the predictions with FSIM instead of PSNR due to the impact of random noise in ground truth EM images (Supplementary Fig.\\xa04).\\nNext, a comprehensive examination was conducted to evaluate the performance of EMDiffuse-n and other denoising methods (Fig.\\xa01b, c). We compared EMDiffuse-n with three widely adopted supervised denoising methods (i.e., CARE28, RCAN39, PSSR23) and two self-supervised methods (i.e., Noise2Noise29 and Noise2Void19). EMDiffuse, as well as the baselines, were trained and tested on a mouse brain cortex dataset acquired in-house (Supplementary Information). EMDiffuse-n surpassed other denoising methods in generating images with intricate ultrastructural information. This allows for adequately distinguishing mitochondria, cellular vesicles, endoplasmic reticulum (ER), and proximal plasma membranes (Fig.\\xa01b, Supplementary Fig.\\xa05, and Supplementary Movie\\xa01 and 2). Example line profiles (Fig.\\xa01b) revealed that EMDiffuse-n successfully separated ultrastructure that is difficult to discriminate, such as mitochondria cristae and proximal plasma membrane, and produced results closest to the GT, while other denoising\\xa0methods (i.e., CARE28, RCAN39, PSSR23) were incompetent in discerning these fine details. Of note, the Fourier power spectrum and resolution measurements40 indicated that EMDiffuse-n generates outputs with high-resolution details (Fig.\\xa01b). By contrast, other supervised and self-supervised deep learning models\\xa0introduced smoothness41 (white line across the center of power spectrum) as they approximated the MMSE of possible true prediction distribution, and partially lost intricate structural details (Fig.\\xa01b and Supplementary Fig.\\xa05). In terms of quantitative evaluations, compared to other denoising methods, EMDiffuse-n demonstrated superior performance across three evaluation metrics, including LPIPS42, FSIM38, and resolution ratio, that assess models‚Äô capability in generating accurate and high-resolution predictions (Fig.\\xa01c).\\nEMDiffuse-n addressed the common concern on the reliability of deep learning-generated data for scientific data processing (Fig.\\xa01d). EMDiffuse-n was designed with the capability to reflect and self-assess the reliability of its predictions by calculating the standard deviation of K outputs for each raw image (Fig.\\xa01a, ‚ÄúMethods‚Äù section). EMDiffuse-n produced low uncertainty values when it was confident about its predictions (Fig.\\xa01d and Supplementary Fig.\\xa06). The overall uncertainty value of each input was calculated (Fig.\\xa01d, ‚ÄúMethods‚Äù section). We performed a study on a dataset with varying noise levels and identified an uncertainty value threshold, 0.12, that could assist biologists and microscopists in assessing the reliability of predictions (Methods). Predictions with higher uncertainty values indicate the high variance between distinct outputs from EMDiffuse and potential inaccuracies in predicted structures.\\nFinally, EMDiffuse-n achieved strong generalizability and transferability (Fig.\\xa01e). The deep learning model‚Äôs performance is susceptible to the domain gap between test and training data, ranging from disparities in the structures to variations in noise levels. We explored the possibility of generalizing and transferring the pre-trained model to new types of biological samples by introducing diverse structures into the model‚Äôs fine-tuning process to mitigate the domain gap (Supplementary Fig.\\xa07a, ‚ÄúMethods‚Äù section). EMDiffuse-n trained on mouse brain cortex dataset was fine-tuned and tested on three different mouse tissue images (i.e., liver, heart, bone marrow) and cultured HeLa cell images acquired in-house (Supplementary Information). EMDiffuse-n had excellent generalizability and high transferability on images from biological samples different from training data (Fig.\\xa01e and Supplementary Movie\\xa03). Moreover, we also showed that the performance of the pre-trained EMDiffuse-n model could be easily improved by fine-tuning the decoder with only a single data pair of images of 3 megapixels from new domains (Supplementary Fig.\\xa07). After fine-tuning, the predictions from all four datasets showed low uncertainty values and high reliability (Fig.\\xa01e). In contrast, hundreds of pairs of training data were required to train the model from scratch on a new domain. This opens the possibility of applying EMDiffuse-n in various EM application scenarios.\\nEMDiffuse achieved superior super-resolution capability\\nWe also applied EMDiffuse for the super-resolution task (EMDiffuse-r) that reconstructs a high-resolution image, which requires a long acquisition time, from a noisy low-resolution image, which needs only a short acquisition time (Fig.\\xa02a and Supplementary Fig.\\xa08). A mouse brain cortex super-resolution dataset composed of paired noisy input with a pixel size of 6.6\\u2009nm and ground truth with a pixel size of 3.3\\u2009nm was obtained in-house to train EMDiffuse-r and other baselines. We optimized the prediction generation and computed the mean of two outputs for each raw input as the prediction (Supplementary Fig.\\xa09a, ‚ÄúMethods‚Äù section) and computed the uncertainty values for assessing reliability. Firstly, EMDiffuse-r has exhibited superior qualitative performance, particularly in improving resolution and resolving intricate details, while other tested methods introduced unwanted smoothness (Fig.\\xa02b, Supplementary Fig.\\xa09b Supplementary Movie\\xa04 and Movie\\xa05), which was consistent from low-resolution input images with different noise levels (Supplementary Fig.\\xa09c). For example, EMDiffuse-r successfully discerned and separated the proximal synaptic vesicles (Fig.\\xa02b) and mitochondria cristae (Supplementary Fig.\\xa09c) while other methods failed to differentiate them. Then, the resolution values and Fourier power spectrum further confirmed that EMDiffuse-r did not introduce the undesirable smoothness and artifacts into predictions (Fig.\\xa02b and Supplementary Fig.\\xa09c). Quantitative comparisons in LPIPS, FSIM, and resolution ratio metrics (Fig.\\xa02c) reveal that EMDiffuse-r outperformed other models in all three metrics and generated the most accurate and high-resolution predictions. The Fourier ring correlation plot further indicates that EMDiffuse captures the intricate details present in the high-frequency components (Fig.\\xa02d). Furthermore, the uncertainty value below the threshold indicated that the prediction was reliable except for extremely noisy raw inputs (Fig.\\xa02b and Supplementary Fig.\\xa09c). The generalizability and robust adaptability of EMDiffuse-r also enabled easy adaptation of the proposed model to a brand-new dataset with a single training data pair of images of 3 megapixels (Fig.\\xa02e, Supplementary Fig.\\xa010, and Supplementary Movie\\xa06). Notably, by super-resolving a noisy 6-nm pixel size image into a clean 3-nm pixel size image, EMDiffuse-r doubled the image resolution and facilitated a 36√ó increase in EM imaging speed for the specific imaging setup used in this study.\\na Schematic of EMDiffuse-r for super-resolution of EM images. b Representative region from mouse brain sections (input with a pixel size of 6.6\\u2009nm, GT with a pixel size of 3.3\\u2009nm) for comparing the super-resolution capability of EMDiffuse-r with CARE, PSSR, and RCAN. Top right of each panel is the Fourier power spectrum. The resolution of processed images is shown on the bottom left of the image. The uncertainty value is shown in the top left corner. c The 3D scatter plot and distribution plots show the quantitative performance assessment of EMDiffuse, CARE, PSSR, and RCAN for the super-resolution task. Each point represents a unique test image of the mouse brain cortex, n\\u2009=\\u2009960. d The Fourier ring correlation plot between predictions in (b) and ground truth image. e EMDiffuse for super-resolution is robust and transferable to other tissues, including the liver, heart, and bone marrow. Resolution values and uncertainty values are indicated on each panel. U uncertainty value. Scale bar, b 0.1\\u2009Œºm; d 0.3\\u2009Œºm. Source data are provided as a Source Data file.\\nvEMDiffuse ‚Äì EMDiffuse for enhancing volume electron microscopy\\nIsotropic resolution reconstruction from anisotropic vEM data can significantly accelerate vEM and expand its potential applications. In this work, we further extended EMDiffuse to vEM data and developed vEMDiffuse, which can generate isotropic volumes from anisotropic ones to reduce the number of layers to be captured for volume generation and accelerate vEM data acquisition (Figs.\\xa03 and 4). Specifically, we developed vEMDiffuse-i which incorporates a channel embedding (‚ÄúMethods‚Äù section) to generate layers between the 1st and Nth layers by learning from a small volume of isotropic training data13,15,43 (Fig.\\xa03a). In the inference phase, vEMDiffuse-i generates vEM data of isotropic resolution from an anisotropic one by generating intermediate layers between two consecutive layers of the anisotropic volume (Fig.\\xa03a, ‚ÄúMethods‚Äù section). Uncertainty values were calculated for predictions of each layer to ensure reliability.\\na Schematic of vEMDiffuse-i. vEMDiffuse-i learns to predict consecutive z-slices (yellow) with the preceding and following slices (green) as input. In the inference stage, vEMDiffuse-i achieves an isotropic resolution of vEM by generating intermediate layers between two layers of anisotropic volume. b Representative generated XY view and the corresponding ground truth (GT) data. c, d Representative vEMDiffuse-i generated YZ views of the Openorganelle mouse kidney dataset (jrc_mus-kidney) and the Openorganelle mouse liver dataset (jrc_mus-liver). Bottom panels are enlarged boxed regions of the anisotropic volume (8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u200948\\u2009nm resolution), interpolation volume, vEMDiffuse-i generated volume, and the ground truth (GT) isotropic volume (8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u20098\\u2009nm resolution). e Comparison of organelle segmentation results between the anisotropic mask (downsampled from ground truth masks, 8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u200948\\u2009nm resolution), the segmentation mask of interpolated volume, vEMDiffuse-i generated volume, and ground truth isotropic volume (upper bound, 8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u20098\\u2009nm resolution) on Openorganelle mouse liver dataset. The Intersection over Union (IoU) was calculated against the ground truth mask. f Reconstructed 3D view based on the segmentation mask of vEMDiffuse-i generated volume (left) and comparison (right) of the 3D view of a zoomed-in volume between the anisotropic mask, the segmentation mask of interpolated volume, the segmentation mask of vEMDiffuse-i generated volume and ground truth mask. Red, mitochondria; blue, endoplasmic reticulum (ER). U uncertainty value. Scale bar, b 2\\u2009Œºm; c, d 1\\u2009Œºm.\\na Schematic of vEMDiffuse-a. During training, vEMDiffuse-a is trained to predict successive y slices (yellow) conditioned on the front and back y layers of anisotropic volume (green). For inference, same as vEMDiffuse-i, vEMDiffuse-a generates middle layers along the z-axis between two consecutive XY images from the anisotropic volume. b Representative vEMDiffuse-a generated XY view of Openorganelle mouse kidney dataset (jrc_mus-kidney). c, d Representative vEMDiffuse-a generated YZ view of Openorganelle mouse kidney dataset (jrc_mus-kidney, c) and Openorganelle mouse liver dataset (jrc_mus-liver, d). Shown are the anisotropic volume (8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u200948\\u2009nm resolution), vEMDiffuse-i reconstruction volume, vEMDiffuse-a reconstruction volume, and isotropic volume (8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u20098\\u2009nm resolution). e, f 3D reconstruction of mitochondria and endoplasmic reticulum of the two datasets. Top panels are the reconstruction of the test volume. The bottom panels exhibit comparisons of an enlarged 3D reconstruction from the anisotropic mask and the segmentation mask of vEMDiffuse-a generated volume. The overlap ratios with the ground truth (IoU) are shown in the top right corner. g, h Representative vEMDiffuse-a generated YZ view (top) and enlarged region (bottom) of the MICrONS multi-area dataset (8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u200940\\u2009nm resolution) and FANC dataset (4\\u2009nm\\u2009√ó\\u20094\\u2009nm\\u2009√ó\\u200940\\u2009nm). U uncertainty value. Scale bar, b 1.5\\u2009Œºm; c, d 0.2\\u2009Œºm; (MICrONS in g), 0.8\\u2009Œºm; (FANC in h), 0.6\\u2009Œºm.\\nWe firstly validated the isotropic reconstruction capability of vEMDiffuse-i using downsampled volumes on two opensource isotropic datasets: OpenOrganelle mouse liver dataset (jrc_mus-liver)44 and OpenOrganelle mouse kidney dataset (jrc_mus-kidney)15. For each dataset, we held out an isotropic subvolume (8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u20098\\u2009nm resolution) for training and downsampled another to an anisotropic volume (8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u200948\\u2009nm resolution) for testing by removing the axial layers. First, the XY view example of the generated liver volume (Fig.\\xa03b, Supplementary Movie\\xa07 and 8) indicated that vEMDiffuse-i could generate a volume with similar ultrastructural information and comparable axial resolution as the original isotropic volume. Then, an examination of a series of XY views showed that vEMDiffuse-i effectively maintained axial continuity and accurately and reliably (i.e., uncertainty values below the uncertainty threshold) replicated the ultrastructural changes of organelles within the isotropic volume (Supplementary Figs.\\xa011, 12, and 13). Further, the YZ views (Fig.\\xa03c, d) and XZ views (Supplementary Fig.\\xa014) of the volume corroborated that vEMDiffuse-i accurately reconstructed the intricate structures of organelles, including mitochondria, ER, and Golgi apparatus along the axial axis. By contrast, these ultrastructural details were lost in anisotropic volume and were not restored in ITK-cubic interpolated45 volume (Fig.\\xa03c, d, and Supplementary Fig.\\xa014). What‚Äôs more, we compared EMDiffuse-i with 3D-SRU-Net46, another supervised deep learning method for isotropic reconstruction (Supplementary Information), using isotropic OpenOrganelle mouse kidney dataset (jrc_mus-kidney). 3D-SRU-Net restored most of the organelles effectively but failed to accurately generate fine structures, such as the Golgi apparatus and mitochondria cristae (Supplementary Fig.\\xa015a, b). Quantitative evaluation using resolution ratio, LPIPS, and FSIM metrics further elaborated that vEMDiffuse-i held an improved performance over 3D-SRU-Net and ITK-cubic interpolation, both in terms of image resolution and fidelity (Supplementary Fig.\\xa015c). Moreover, we also trained and applied vEMDiffuse-i on the OpenOrganelle T-cell47 (jrc_ctl-id8-2, Supplementary Figs.\\xa016 and 17) and EPFL mouse brain datasets (Supplementary Fig.\\xa018)48. vEMDiffuse-i consistently generated high-quality results with low uncertainty values and accurate organelle ultrastructure (Supplementary Fig.\\xa016 and Supplementary Fig.\\xa018a), suggesting its universal applicability on a variety of vEM datasets with a small volume of isotropic training data.\\nSegmentation of ultrastructural details (e.g., organelles) from vEM data is an essential task for vEM analysis, which often requires high-quality isotropic volumes43. Herein, we showed that vEMDiffuse-i predictions from anisotropic input volumes enabled accurate segmentation of organelles, achieving a similar quality as that using the isotropic resolution vEM captured with FIB-SEM as input (Fig.\\xa03e, f). We trained mitochondria and ER segmentation models on the isotropic OpenOrganelle mouse liver volume with corresponding masks (Supplementary Fig.\\xa019, ‚ÄúMethods‚Äù section)43,49. The trained models were applied to interpolated volume, vEMDiffuse-i generated volume, and ground truth isotropic volume (‚ÄúMethods‚Äù section). The vEMDiffuse-i generated volume achieved a similar Intersection over the Union (IoU) score as isotropic volume (Fig.\\xa03e), indicating precise organelle prediction by vEMDiffuse-i. In contrast, the IoU scores of the anisotropic volume and interpolated volume were low (Fig.\\xa03e). The 3D rendering visualizations (Fig.\\xa03f and Supplementary Movie\\xa09) demonstrated the accurate reconstruction of organelle ultrastructure in vEMDiffuse-i generated volume. In contrast, anisotropic volume and interpolated volume failed the task, which represents a major limitation of serial section-based vEM techniques.\\nWe evaluated the capability of vEMDiffuse-i to generate isotropic volumes from anisotropic volumes of different axial resolutions from 48\\u2009nm to 96\\u2009nm (Supplementary Fig.\\xa020). vEMDiffuse-i successfully generated fine structures of organelles with axial resolution up to 64\\u2009nm, demonstrated by high IoU scores of ER and mitochondria segmentation masks above 0.9 (Supplementary Fig.\\xa020a) and uncertainty values below the uncertainty threshold (Supplementary Fig.\\xa020b). With axial resolution of 96\\u2009nm, we observed increased artifacts in generated volume, reduced IoU scores of segmentation masks (Supplementary Fig.\\xa020a), increased uncertainty values above the uncertainty threshold (Supplementary Fig.\\xa020b), and decreased FSIM metrics (Supplementary Fig.\\xa020c).\\nvEMDiffuse reconstructs isotropic resolution vEM volumes without isotropic resolution vEM training data\\nAcquiring high-quality isotropic vEM data from FIB-SEM is often impractical for most research laboratories, and the total volume that can be imaged in 3D by FIB-SEM is often small. To democratize access to isotropic vEM data and enable isotropic reconstruction of large tissue samples, we further developed vEMDiffuse-a which reconstructed isotropic volumes with only anisotropic training data (Fig.\\xa04a). The division of input and target volumes for producing the training data in vEMDiffuse-a was the same as that of vEMDiffuse-i but occurred along the y-axis (as opposed to the z-axis in vEMDiffuse-i) (‚ÄúMethods‚Äù section). The inference stage was identical to vEMDiffuse-i, where vEMDiffuse-a generated intermediate layers between two consecutive XY layers of an anisotropic volume.\\nTo consolidate our assumption that information in the lateral axis is transferable to the axial axis, we first trained and tested vEMDiffuse-a on the manually downgraded anisotropic OpenOrganelle mouse kidney (jrc_mus-kidney) dataset15 and downgraded anisotropic OpenOrganelle mouse kidney (jrc_mus-liver) dataset, where some layers along the z-axis have been removed to simulate an anisotropic vEM data (8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u200948\\u2009nm resolution, ‚ÄúMethods‚Äù section). First, although the XZ view training images contain undesired distortions (Supplementary Fig.\\xa021a), we find that the vEMDiffuse-a model, given two consecutive XY view images as input, could produce intermediate layers with similar quality and resolution as the ground truth ones (Fig.\\xa04b, Supplementary Fig.\\xa021b, Supplementary Movie\\xa010 and 11). The uncertainty value of generated layers also indicated that the predictions were reliable (Supplementary Fig.\\xa021b). Further, the YZ and XZ views of the generated isotropic volume demonstrated that vEMDiffuse-a accurately captured the lateral information and leveraged it to enhance the axial resolution (Fig.\\xa04c, d Supplementary Fig.\\xa022 and Supplementary Movie\\xa012). For example, the continuity of ER and contacts between ER and mitochondria (Fig.\\xa04c, d) was accurately restored by vEMDiffuse-a. In contrast, in anisotropic volume, such information was lost, limiting investigations of organelle-organelle interactions in 3D. We further compared vEMDiffuse-a with CARE (Supplementary Information), neither of which required isotropic training volumes. Specifically, CARE and vEMDiffuse-a were trained and tested on downsampled 8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u200948\\u2009nm anisotropic Openorganelle mouse kidney volumes (jrc_mus-kidney). CARE and ITK-cubic interpolation faced challenges with fine ultrastructure (Supplementary Fig.\\xa023a, b). The quantitative evaluation using resolution ratio, LPIPS, and FSIM again elaborated the superior capabilities in generating isotropic volumes of vEMDiffuse-a (Supplementary Fig.\\xa023c). Moreover, vEMDiffuse-a improved vEM organelle reconstruction, which will facilitate 3D investigation at the organelle level from an anisotropic volume (Fig.\\xa04e, f and Supplementary Movie\\xa013). Again, anisotropic volume failed to reconstruct accurate 3D structures of mitochondria and ER. The mitochondria membrane segmentation and reconstruction on the Openorganelle mouse liver dataset (jrc_mus-liver) also demonstrated vEMDiffuse-a‚Äôs capability to facilitate 3D ultrastructural analysis (Supplementary Fig.\\xa024).\\nvEMDiffuse-a can be trained on any existing tissue array tomography type of anisotropic volumes and produce reliable isotropic volumes. We applied vEMDiffuse-a to two large open anisotropic tissue array tomography-type datasets: the MICrONS multi-area dataset50 (Fig.\\xa04g) and the FANC dataset17 (Fig.\\xa04h). vEMDiffuse-a could generate an 8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u20098\\u2009nm voxel isotropic volume from the 8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u200940\\u2009nm resolution MICrONS multi-area volume (Fig.\\xa04g, Supplementary Fig.\\xa025) and a 4\\u2009nm\\u2009√ó\\u20094\\u2009nm\\u2009√ó\\u20094\\u2009nm voxel volume from the 4\\u2009√ó\\u20094\\u2009√ó\\u200940\\u2009nm FANC volume (Fig.\\xa04h, Supplementary Fig.\\xa026). The lateral image quality was comparable to the original volume (Supplementary Movie\\xa014 and 15) with uncertainty values below the threshold (Supplementary Fig.\\xa025b, Supplementary Fig.\\xa026b), while the axial resolution was boosted to an isotropic level, enabling researchers to observe seamless organelle ultrastructural changes along the z-axis with tissue array tomography data (Fig.\\xa04g, h and Supplementary Movie\\xa016). By reconstructing an anisotropic volume into an isotropic one, vEMDiffuse-a enabled the visualization of cellular structures in 3D, paving the way for researchers to map the isotropic 3D ultrastructure of organelles within large tissue samples and pushing the boundaries of applications with vEM.\\nWe demonstrated the application of vEMDiffuse-a in neurite tracing on the downsampled MANC dataset51 (8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u200948\\u2009nm resolution), using a Flood-Filling Network (FFN)52 model (Supplementary Information). Indicated by high IoU scores, vEMDiffuse-a generated volume (Supplementary Fig.\\xa027a) showed improved capabilities in generating fine neurite details and capturing complex branching structures, when compared to the over-smoothed anisotropic masks and the segmentation masks of interpolated volume (Supplementary Fig.\\xa027b).\\nDiscussion\\nThis study introduces EMDiffuse, a suite of deep learning-based methods that enhance the imaging power of ultrastructural imaging with EM and vEM. EMDiffuse expedites imaging processes and improves imaging quality through denoising and super-resolution tasks. More importantly, EMDiffuse enables robust isotropic vEM volume generation, even without the isotropic training data.\\nLeveraging the diffusion model, EMDiffuse exhibited superior performance in augmenting EM imaging while generating the images with nanoscale biological structure details. Contrary to the baseline models that predict the average or median of all feasible solutions, thereby inducing smoothness, EMDiffuse opts to sample a single plausible solution at each testing instance. Consequently, it enables users to produce unlimited high-resolution images for examinations of areas of interest. In our experiments, we generated two predictions and used their mean as the final output. Of note, the outputs of different test times might exhibit variability with notably noisy inputs. We have configured the EMDiffuse to allow the generation of multiple plausible solutions for future users to analyze them individually.\\nMoreover, EMDiffuse attempted to address the reliability conundrum in bioimage processing deep learning applications by assessing the variance between different outputs. We have provided a reference threshold of 0.12, which has been applicable in assessing all tasks demonstrated in this work. However, considering the wide application range of electron microscopy, establishing a universally applicable threshold remains an elusive goal. Future users are encouraged to fine-tune the threshold by analyzing their dataset‚Äôs uncertainty maps and predictions.\\nEMDiffuse demonstrated robustness and transferability to facilitate adaptation to other EM datasets with only one pair of fine-tuning images. Practically, microscopists may acquire a single training pair of images of 3 megapixels comprising a clean and noisy image of the target tissue to fine-tune the model. Of note, we observed the differences in the denoising performance for the direct application without fine-tuning, which likely represents the in-domain constraint of the deep learning model. The number of pixels or images used for fine-tuning the model will vary depending on how different the target dataset is from the training dataset.\\nIn our experiments, EMDiffuse can faithfully restore high-quality images from noisy images and reduce the data acquisition time by a factor of 18. In the super-resolution task, EMDiffuse reduced the data acquisition time by a factor of 36 by restoring noisy and lower-resolution images to high-quality and resolution images. However, the expedition factor may vary among different EM instruments, depending on the instrument‚Äôs setup, particularly the detectors‚Äô sensitivity. Nevertheless, it is possible to combine the acceleration capabilities of EMDiffuse and state-of-the-art EM instruments to push the limitations of attainable imaging areas of EM ‚Äì potentially more biological insights within the same acquisition time.\\nvEMDiffuse broadens the capabilities of vEM imaging. EMDiffuse-i and EMDiffuse-a were designed to tackle vEM data from the most commonly used vEM techniques, FIB-SEM, and serial section-based tomography techniques. EMDiffuse-i demonstrated its robustness with isotropic generation of four volumes from distinctive biological samples of various nanoscale structures and contrast (i.e., mouse brain, mouse kidney, mouse liver, and cultured T-cell and cancer cell). vEMDiffuse-a transfers the knowledge acquired from the anisotropic volume lateral axes to the axial axes. It can be applied to any extant tissue array tomography-type anisotropic vEM data from serial section-based techniques without additional data for training. Isotropic reconstruction and axial resolution enhancement on two public extensive vEM datasets‚ÄìMICrONS multi-area and FANC‚Äìhave exemplified vEMDiffuse-a‚Äôs capability to perform universal image transformations in existing large datasets. vEMDiffuse-a enables accurate isotropic reconstruction at the organelle scale for serial section-based tomography vEM datasets, enhancing our ability to perform the three-dimensional reconstruction of numerous nanoscale organelles within tissues that were unfeasible to investigate. The combination of vEMDiffuse-a and serial section-based tomography methods is poised to democratize the isotropic vEM.\\nAlthough the inherent model of vEMDiffuse is 2D, we incorporated axial information into the 2D diffusion model through channel embedding. This technique enables the model to accurately discern and generate specific layers within the 3D structure. What‚Äôs more, compared with the 3D model, the 2D network is much more computationally and memory efficient, which makes it feasible to increase the size of the network (e.g., the number of layers) and enhance the capability of the model to process complex biological structures.\\nThe capability of EMDiffuse is dependent on the axial resolution of the anisotropic volume, which is recommended to be as fine as the diameter of interested structures to reconstruct. For example, an axial resolution below 56\\u2009nm can reliably reconstruct fine ultrastructural details of organelles, including ER and mitochondria cristae. We expect that vEMDiffuse can also reconstruct other fine structures accurately, given an axial resolution that approximates the sizes of interested structures, such as synaptic vesicles and synapses, to facilitate connectomics studies. A poorer axial resolution significantly risks fine structures being indiscernible or inaccurately reconstructed, appearing smeared across successive layers, thus undermining the reconstruction.\\nFurther refinements to EMDiffuse are anticipated to fully harness the potential impact of this deep learning model on biological imaging and cell biology. The stable diffusion model33 could be adopted as the backbone of EMDiffuse to accelerate the inference processes. The integration of algorithms of the EMDiffuse package can be investigated for further enhancements of EM imaging, particularly to accelerate the data acquisition and reconstruction of large isotropic volumes. The continued development of EMDiffuse will pave the way for investigations of three-dimensional ultrastructural changes in large volumes of biological systems with isotropic resolution.\\nMethods\\nDenoise data pre-processing\\nThe inherent distortions and horizontal-and-vertical drifts associated with EM imaging53 may result in misaligned training data (Supplementary Fig.\\xa01a). Therefore, it is crucial to align the data properly for training the EMDiffuse model. In this study, we presented a two-stage registration strategy for coarse-to-fine alignment of the raw image and the ground truth reference image. The coarse stage employed a key-point matching-based registration technique54, which estimated a global homography matrix between them based on ORB (Oriented FAST and Rotated BRIEF) key points and feature descriptors55. The homography matrix was used to warp the raw image and aligned it with the reference image. However, the coarse stage could only align the two images globally, and local misalignments still existed (Supplementary Fig.\\xa01b). Hence, in the fine stage, we employed optical flow estimation56 to estimate dense correspondences between pixels in two image patches which were further used to warp the raw image to achieve better local alignment with the reference image.\\nAdditionally, for the sake of training efficiency, we cropped 256\\u2009√ó\\u2009256 image patches with a stride of 196 to construct the data for training and inference, resulting in around 20,000 patches for training, 1000 patches for validation, and 1000 patches for inference. In the inference stage, for one raw image, the corresponding output patches from the model were stitched together using Imaris Stitcher (Oxford Instruments Group) to be the final output.\\nEMDiffuse architecture, diffusion process, and training objective\\nThe architecture of UDiM and the diffusion process of EMDiffuse are included in Supplementary Information. We observed that EMDiffuse training often got stuck at the hard cases (e.g., extremely noisy raw images in the denoising task). Therefore, inspired by an existing method37, we added an additional prediction head that produced a difficulty assessment map \\\\({{{{{{\\\\boldsymbol{\\\\varphi }}}}}}}_{i}\\\\) and used it to weight the prediction error. This resulted in a difficulty-aware loss function37:\\nWhere \\\\({{{{{{\\\\bf{x}}}}}}}_{0}\\\\) is the ground truth, \\\\({{{{{{\\\\bf{c}}}}}}}_{r}\\\\) is the raw input image, \\\\({{{{{\\\\boldsymbol{\\\\epsilon }}}}}}{{{{{\\\\boldsymbol{ \\\\sim }}}}}}{{{{{\\\\mathscr{N}}}}}}({{{{\\\\mathrm{0,1}}}}})\\\\), \\\\({{{{{{\\\\boldsymbol{\\\\epsilon }}}}}}}_{\\\\theta }\\\\) is the network, t is the timepoint, \\\\({\\\\bar{\\\\alpha }}_{t}={\\\\prod }_{i=1}^{t}{\\\\alpha }_{i}\\\\) and \\\\({\\\\alpha }_{i}\\\\) is the noise schedule (more details are included in Supplementary Information).\\nInference stage\\nTo generate samples from the target distribution, we start from a random noise sample \\\\({x}_{T}\\\\) and iteratively apply the denoising function \\\\({p}_{\\\\theta }\\\\left({x}_{t-1} | {x}_{t}\\\\right)\\\\) in reverse order, from step T to step 0:\\nAlso, each step is also guided by the raw input \\\\({{{{{{\\\\boldsymbol{c}}}}}}}_{r}\\\\). Through step-by-step denoising, we can approximate the target distribution \\\\(p({x}_{0})\\\\) in the final step \\\\({x}_{0}\\\\) as the final prediction from the diffusion model. As for uncertainty prediction, we adopt the last step (i.e., t\\u2009=\\u20090) of the diffusion process as the final variance map.\\nDenoising neural networks training\\nFive neural networks, CARE28, RCAN39, PSSR23, Noise2Noise29, and Noise2Void19, were compared with our EMDiffuse-n on the denoising task. Each model was trained and tested on the aligned and cropped multiple noise levels mouse brain cortex SEM dataset. To prevent overfitting, the training was stopped after the validation loss did not decrease for 20 epochs. The checkpoint with the best performance on the validation set was selected for testing. Data augmentation techniques such as random flip and rotation were applied during training. All experiments were done on a computer workstation equipped with four RTX 3090Ti NVIDIA graphic processing cards. All the plots were generated using Matplotlib and Seaborn libraries in Python. More details of baseline implementation are included in Supplementary Information.\\nEMDiffuse-n implementation was done using Python version 3.8 and Pytorch version 1.7.057. The Adam optimizer58 with an initial learning rate of 5e-5 was employed to train EMDiffuse-n. The learning rate remained constant throughout the training processes since we observed no overfitting except in transfer learning experiments. The batch size was 64. Gaussian blur was included in data augmentation, which proved beneficial in SR359 (Supplementary Table\\xa01). Given paired high-quality image \\\\({{{{{{\\\\bf{x}}}}}}}^{{gt}}\\\\) and noisy image \\\\({{{{{{\\\\boldsymbol{c}}}}}}}_{n}\\\\), the training objective for EMDiffuse-n is:\\nOptimization of prediction generation\\nWhen the noise level of a raw image is high, denoising and super-resolution tasks become notorious ill-posed one-to-many problems. In regions where noise dominates, multiple potential solutions exist (Supplementary Fig.\\xa03a). To model such inherent ambiguities, we harnessed the power of diffusion models in sampling one plausible solution at each test time. Specifically, given a single raw image \\\\({{{{{{\\\\bf{c}}}}}}}_{r}\\\\), we sampled from the learned solution distribution multiple times to produce inputs \\\\(\\\\{{{{{{{\\\\bf{x}}}}}}}_{T}^{1},{{{{{{\\\\bf{x}}}}}}}_{T}^{2},\\\\ldots .,{{{{{{\\\\bf{x}}}}}}}_{T}^{K}\\\\}\\\\) for the diffusion model, which in turn produces multiple results \\\\(\\\\{{{{{{{\\\\bf{x}}}}}}}_{0}^{1},{{{{{{\\\\bf{x}}}}}}}_{0}^{2},\\\\ldots .,{{{{{{\\\\bf{x}}}}}}}_{0}^{K}\\\\}\\\\). These diverse outputs captured the inherent multi-modal solutions. To alleviate the ambiguities in the predictions, we used the mean of these diverse outputs to be the final prediction \\\\({{{{{{\\\\bf{x}}}}}}}_{0}\\\\).\\nAs the task becomes increasingly ill-posed with a higher noise level in the raw input image, we increased the number of \\\\(K\\\\) during inference. However, if the noise level of the raw image is low, incorporating a larger \\\\(K\\\\) to calculate the mean may introduce undesired smoothness into the prediction, potentially undermining the prediction resolution (Supplementary Fig.\\xa03c, d). Therefore, to achieve a balance of image quality and prediction robustness, we empirically determined the optimal number of K for a given input based on its noise level using the multi-noise-level mouse brain denoise dataset. We processed the data with different values of K and evaluated the quality of the outputs using FSIM as the metric (Supplementary Fig.\\xa04a). We observed that K\\u2009=\\u20092 achieved superior FSIM scores in most cases and enhanced the predictions in most noise cases (noise level\\u2009>\\u200950). At extreme noise cases, using a larger K might contribute only marginal enhancement, and the resultant outputs might lack reliability, as the raw image is too noisy to restore some intricate structures (Supplementary Fig.\\xa06). Users should be cautious about the predictions if the input is excessively noisy. This could also be reflected in our uncertainty score as described in the following. We measured the noise level of the raw image using BRISQUE which will be covered later.\\nAssessing the reliability of predictions\\nUncertainty value. Estimating the epistemic uncertainty of the model predictions is vital for enhancing reliability and aiding biologists in making informed decisions. We employed the variance of the K outputs of the diffusion model as the model‚Äôs epistemic uncertainty (Supplementary Fig.\\xa06), as the prediction becomes increasingly trustworthy if multiple outputs agree. Our methodology evaluates uncertainty on a patch-by-patch basis for large images. Specifically, we divide large images into smaller patches of 256\\u2009√ó\\u2009256\\u2009pixels. For each patch, min-max normalization was applied to enhance the local contrast. Then, we calculated the standard deviation (STD) of K outputs \\\\(\\\\{{{{{{{\\\\bf{x}}}}}}}_{0}^{1},{{{{{{\\\\bf{x}}}}}}}_{0}^{2},\\\\ldots .,{{{{{{\\\\bf{x}}}}}}}_{0}^{K}\\\\}\\\\) (K\\u2009=\\u20092 in our experiments):\\nWhere the \\\\({{{{{{\\\\bf{x}}}}}}}_{0}\\\\) is the average of outputs. Given the uncertainty map \\\\({{{{{{\\\\boldsymbol{\\\\delta }}}}}}}_{{{{{{\\\\rm{i}}}}}}},\\\\) we obtained the uncertainty value of the prediction by extracting the 99th percentile pixel value of \\\\({{{{{{\\\\boldsymbol{\\\\delta }}}}}}}_{{{{{{\\\\rm{i}}}}}}}.\\\\) This percentile was chosen to exclude unreliable predictions even when the model was uncertain only in a minor section of the image. Instead of using the maximum pixel value, this strategy helped avoid undesired high uncertainty values caused by outliers and high contrast pixels. The high contrast indicates sudden shifts in pixel intensities which often happens in structural boundaries, such as organelle membranes will induce elevated standard deviations and thus high uncertainty value (Supplementary Fig.\\xa06). However, this does not necessarily mean incorrect predictions of those regions. After empirical analysis of approximately 200 image predictions and their corresponding ground truths from our mouse brain cortex denoising dataset, 1% of the maximum uncertainty pixels were empirically disregarded in uncertainty estimation to avoid the above issues (Supplementary Fig.\\xa06). Nevertheless, because electron microscopy has a wide range of applications, the quantity of disregarded pixels may vary across different datasets and can be modified by users through examination of the uncertainty map and estimation of the unreasonably high number of uncertain pixels. Upon reassembly, if any individual patch exceeds the predetermined uncertainty threshold, EMDiffuse flags the entire image and the specific patch as potentially problematic.\\nUncertainty threshold\\nWe established an uncertainty threshold \\\\(\\\\tau\\\\) to guide biologists in practice. For one input, the prediction was considered reliable if the uncertainty value 99th percentile pixel value of \\\\({{{{{{\\\\boldsymbol{\\\\delta }}}}}}}_{i}\\\\)) was below \\\\(\\\\tau\\\\). We determined the uncertainty threshold by examining the prediction accuracies of our multi-noise-level mouse brain denoise dataset using FSIM and LPIPS. We observed there is a transition valley between the low LPIPS and high LPIPS regions (Fig.\\xa01c), which we found corresponds to images that are at the boundary of being accurate predictions (low LPIPS and high FSIM). We thus calculated the uncertainty of these samples and used their average to be the uncertainty threshold \\\\(\\\\tau\\\\) which was found to be 0.12.\\nEMDiffuse-n transfer learning\\nEMDiffuse-n data-efficient fine-tuning. First, we examined EMDiffuse-n‚Äôs generalization capability by directly applying the pre-trained model from the mouse brain cortex dataset to these new datasets (Fig.\\xa01e). Second, we investigated the effectiveness of transferring the pre-trained EMDiffuse-n model to new tissues in a data-efficient manner by fine-tuning the model using few-shot tissue samples. To help the model avoid overfitting to the limited training samples and achieve better performance, we only fine-tuned the decoder and bottleneck layer parameters, as opposed to all model parameters (Supplementary Fig.\\xa07a). We also investigated the performance of the model after fine-tuning using the different numbers of training samples (Supplementary Fig.\\xa07b).\\nEMDiffuse-n fine-tuning details\\nDuring training, we used a learning rate of 1e-5 and fine-tuned EMDiffuse-n over 200 epochs on each dataset. All other configurations, such as the optimizer, remained consistent with those used in training EMDiffuse-n. To eliminate the impact of randomness, all experiments were replicated five times using the same training configurations.\\nEMDiffuse-r for super-resolution\\nIn the case of EMDiffuse-r, raw images were captured with a magnification factor of 20,000√ó with a pixel size of 6.6\\u2009nm, utilizing dwell time of 2\\u2009¬µs, 4\\u2009¬µs, and 6\\u2009¬µs. The ground truth reference images were acquired at a 40,000√ó magnification factor with a pixel size of 3.3\\u2009nm and 36\\u2009¬µs dwell time, covering half the region size of the raw images. Images were acquired using the FEI Verios SEM with an acceleration voltage of 2\\u2009kV. For data pre-processing, the same registration and cropping pipeline were employed, resulting in 15,000 well-aligned pairs for training EMDiffuse-r and other super-resolution models, 800 pairs for validation, and 800 pairs for testing. The raw, low-resolution \\\\(128\\\\times 128\\\\) image patch was rescaled to match its target high-resolution size of \\\\(256\\\\times 256\\\\). The same training strategy and the same loss function were adopted as EMDiffuse-n. Similarly, we set parameter \\\\(K\\\\) to 2 for calculating the mean and uncertainty map as output (Supplementary Fig.\\xa09a). The loss function for EMDiffuse-r was:\\n\\\\({{{{{{\\\\bf{x}}}}}}}^{{gt}}\\\\) was high-resolution image and \\\\({{{{{{\\\\boldsymbol{c}}}}}}}_{n}\\\\) was rescaled low-resolution image.\\nEMDiffuse-r transfer learning\\nTo validate the transferability of EMDiffuse-r, we transferred EMDiffuse-r trained on mouse brain cortex dataset to mouse liver, mouse heart, mouse bone marrow, and cultured HeLa cell datasets (the same datasets in EMDiffuse-n transfer learning). Since these three datasets were designed for transfer learning in the denoising task, the raw input was acquired with the same magnification rate as GT. Inspired by PSSR23, we addressed the scarcity of low-resolution images by manually downsampling the raw input with a short acquisition time. However, unlike PSSR, since we already had noisy images with low dwell time, we didn‚Äôt have to manually add Gaussian noise. Remarkably, we also achieved superior performance with a single pair of noisy downsampled images and GT (Fig.\\xa02d) with the previously introduced transfer learning method.\\nSuper-resolution baseline training\\nFor the super-resolution task, CARE28, RCAN39, PSSR23 were compared with EMDiffuse-r. Each model was trained and tested on the same well-aligned brain cortex dataset consisting of low-resolution noisy input and high-resolution ground truth image. The low-resolution image was rescaled to match the target image size. The implementation and training details were kept the same as the denoising task.\\nvEMDiffuse-i for isotropic reconstruction\\nvEMDiffuse-i was designed to restore an isotropic volume (voxel size 8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u20098\\u2009nm in our experiments) from an anisotropic one (voxel size 8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u200948\\u2009nm in our experiments) by interpolating intermediate layers \\\\(\\\\{{{{{{{\\\\bf{x}}}}}}}_{0}^{1},{{{{{{\\\\bf{x}}}}}}}_{0}^{2},\\\\ldots .,{{{{{{\\\\bf{x}}}}}}}_{0}^{R}\\\\}\\\\) between two adjacent layers \\\\({{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{u}}}}}}}\\\\) (the upper layer), \\\\({{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{l}}}}}}}\\\\) (the lower layer) in an anisotropic volume (Fig.\\xa03a).\\nChannel embedding\\nAs the axial resolution of vEM fluctuates across various datasets, channel embedding ensures the model can produce the appropriate number of interpolation layers (R) without architectural modification, which allows model architecture to be reused across different datasets. The number of interpolation layers R is determined by the resolution of the training dataset to generate isotropic volumes. Channel embedding in vEMDiffuse employs sinusoidal positional encoding to embed an integer \\\\({{{{{\\\\rm{j}}}}}}\\\\) \\\\({{{{{\\\\rm{\\\\epsilon }}}}}}[1,{{{{{\\\\rm{R}}}}}}]\\\\) into a feature. This integer signifies the relative position between the target layer and the upper layer \\\\({{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{u}}}}}}}.\\\\) vEMDiffuse is designed to produce one layer per iteration. In each training iteration, an integer \\\\({{{{{\\\\rm{j}}}}}}\\\\) \\\\({{{{{\\\\rm{\\\\epsilon }}}}}}[1,{{{{{\\\\rm{R}}}}}}]\\\\) is randomly selected as the input for channel embedding. The model is then trained to generate the \\\\({{{{{{\\\\rm{j}}}}}}}^{{{{{{\\\\rm{th}}}}}}}\\\\) layer between the upper layer \\\\({{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{u}}}}}}}\\\\) and the lower layer \\\\({{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{l}}}}}}}\\\\). In the inference stage, vEMDiffuse employs channel embedding to embed the index ranging from 1 to R and generates corresponding intermediate layers sequentially.\\nvEMDiffuse-i training\\nDuring each training iteration, vEMDiffuse-i randomly selected one layer as the upper layer \\\\({{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{u}}}}}}}\\\\) in training isotropic volume and concatenated it with \\\\({{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{u}}}}}}+{{{{{\\\\rm{R}}}}}}+1}\\\\) layer of the volume (i.e., \\\\({{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{l}}}}}}}{{{{{\\\\boldsymbol{=}}}}}}{{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{u}}}}}}+{{{{{\\\\rm{R}}}}}}+1}\\\\)). The model was required to generate \\\\(\\\\{{{{{{{\\\\bf{x}}}}}}}_{0}^{{{{{{\\\\rm{u}}}}}}+1},{{{{{{\\\\bf{x}}}}}}}_{0}^{{{{{{\\\\rm{u}}}}}}+2},\\\\ldots .,{{{{{{\\\\bf{x}}}}}}}_{0}^{{{{{{\\\\rm{u}}}}}}+{{{{{\\\\rm{R}}}}}}}\\\\}\\\\) layers of the volume. The training objective of vEMDiffuse-i was:\\nNo overfitting was observed in our dataset, and we chose the checkpoint when vEMDiffuse-i‚Äôs performance stabilized on the validation set. For each dataset, we trained a model for it.\\nvEMDiffuse-i inference\\nDuring the inference stage, given an anisotropic volume \\\\(\\\\{{{{{{{\\\\bf{c}}}}}}}^{1},{{{{{{\\\\bf{c}}}}}}}^{2},\\\\ldots .,{{{{{{\\\\bf{c}}}}}}}^{M}\\\\}\\\\) with M layers, we traverse every pair of adjacent layers{\\\\({{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{u}}}}}}}\\\\), \\\\({{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{u}}}}}}+1}\\\\)} of the volume: [\\\\(\\\\{{{{{{{\\\\bf{c}}}}}}}^{1},{{{{{{\\\\bf{c}}}}}}}^{2}\\\\},\\\\{{{{{{{\\\\bf{c}}}}}}}^{2},{{{{{{\\\\bf{c}}}}}}}^{3}\\\\},\\\\ldots .,\\\\{{{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{M}}}}}}-1},{{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{M}}}}}}}\\\\}\\\\)]. For each pair of input, vEMDiffuse-i generated R images \\\\(\\\\{{{{{{{\\\\bf{x}}}}}}}_{0}^{1},{{{{{{\\\\bf{x}}}}}}}_{0}^{2},\\\\ldots .,{{{{{{\\\\bf{x}}}}}}}_{0}^{{{{{{\\\\rm{R}}}}}}}\\\\}\\\\) between them:\\nThen we inserted the generated layers between \\\\(\\\\{{{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{u}}}}}}},\\\\,{{{{{{\\\\bf{c}}}}}}}^{{{{{{\\\\rm{u}}}}}}+1}\\\\}\\\\). Thus, the final generated volume \\\\(\\\\{{{{{{{\\\\bf{c}}}}}}}^{1},{{{{{{{\\\\bf{x}}}}}}}_{0}^{1},{{{{{{\\\\bf{x}}}}}}}_{0}^{2},\\\\ldots .,{{{{{\\\\bf{c}}}}}}}^{2},\\\\ldots .,{{{{{{\\\\bf{c}}}}}}}^{M}\\\\}\\\\) had ((M ‚àí1) \\\\(\\\\times\\\\) R\\u2009+\\u20092) layers. To enhance the performance of vEMDiffuse-i, we obtained two outputs (K\\u2009=\\u20092) for each input and use their mean as the final prediction \\\\({{{{{{\\\\bf{x}}}}}}}_{0}^{{{{{{\\\\rm{j}}}}}}}\\\\).\\nvEMDiffuse-i inference dataset preparation\\nIn the inference stage, we firstly cropped 1024\\u2009√ó\\u20091024\\u2009√ó\\u20091024 subvolume (not used in the training stage) from OpenOrganelle Kidney isotropic volume15 and OpenOrganelle Liver isotropic volume44 as test datasets (voxel size is 8\\u2009nm\\u2009√ó\\u20098\\u2009nm\\u2009√ó\\u20098\\u2009nm). We manually removed some layers to simulate the anisotropic volumes. Starting from \\\\({{{{{{\\\\bf{c}}}}}}}^{1}\\\\), we retained only the layers with indices whose remainder is 1 after divided by 6 while discarding the others:\\nmod means the remainder after division. This reduces the axial voxel size from 8\\u2009nm to 48\\u2009nm.\\nvEMDiffuse-a for isotropic reconstruction\\nIn vEMDiffuse-i, we extracted layers along the z-axis of isotropic volume to train, and thus we required isotropic volume for training. However, obtaining isotropic volume is impractical for many research groups. In vEMDiffuse-a, we trained the model‚Äôs isotropic reconstruction only with anisotropic volume.\\nvEMDiffuse-a model training\\nIn vEMDiffuse-a, we extracted layers along the y-axis of the anisotropic volume (equivalent to the z-axis of isotropic volume in vEMDiffuse-i) to generate input and target pair to train (Fig.\\xa04a). In the training stage, vEMDiffuse-a was required to generate \\\\(\\\\{{{{{{{\\\\bf{xz}}}}}}}_{0}^{{{{{{\\\\rm{u}}}}}}+1},{{{{{{\\\\bf{xz}}}}}}}_{0}^{{{{{{\\\\rm{u}}}}}}+2},\\\\ldots .,{{{{{{\\\\bf{xz}}}}}}}_{0}^{{{{{{\\\\rm{u}}}}}}+{{{{{\\\\rm{R}}}}}}}\\\\}\\\\) XZ layers of the volume given input pair of upper and lower XZ layer pair \\\\(\\\\{{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xz}}}}}}}^{{{{{{\\\\rm{u}}}}}}},{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xz}}}}}}}^{{{{{{\\\\rm{l}}}}}}}\\\\}\\\\). Of note, \\\\(\\\\{{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xz}}}}}}}^{{{{{{\\\\rm{u}}}}}}},{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xz}}}}}}}^{{{{{{\\\\rm{l}}}}}}}\\\\}\\\\) and \\\\(\\\\{{{{{{{\\\\bf{xz}}}}}}}_{0}^{{{{{{\\\\rm{u}}}}}}+1},{{{{{{\\\\bf{xz}}}}}}}_{0}^{{{{{{\\\\rm{u}}}}}}+2},\\\\ldots .,{{{{{{\\\\bf{xz}}}}}}}_{0}^{{{{{{\\\\rm{u}}}}}}+{{{{{\\\\rm{R}}}}}}}\\\\}\\\\) are both XZ view images exhibiting reduced image quality with line artifacts (Supplementary Fig.\\xa025a and Supplementary Fig.\\xa026a). We find the model is robust to these artifacts. We used the same training strategy as vEMDiffuse-i, but we noticed severe overfitting after a long training time, resulting in XZ-like predictions with clear line artifacts. To avoid overfitting, we terminated the training after 1200 epochs. Again, for each dataset, we trained a model for it.\\nvEMDiffuse-a inference\\nThe inference stage of vEMDiffuse-a was equivalent to that of vEMDiffuse-i. vEMDiffuse-a interpolated intermediate layers between upper and lower input pair \\\\(\\\\{{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xy}}}}}}}^{{{{{{\\\\rm{u}}}}}}},{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xy}}}}}}}^{{{{{{\\\\rm{l}}}}}}}\\\\}\\\\). Again, of note, in the training stage, the input \\\\(\\\\{{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xz}}}}}}}^{{{{{{\\\\rm{u}}}}}}},\\\\,{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xz}}}}}}}^{{{{{{\\\\rm{l}}}}}}}\\\\}\\\\) were XZ view images and the model was trained to interpolate layers along the y-axis. However, in the inference stage, the input \\\\(\\\\{{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xy}}}}}}}^{{{{{{\\\\rm{u}}}}}}},\\\\,{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xy}}}}}}}^{{{{{{\\\\rm{l}}}}}}}\\\\}\\\\) were XY view images of the anisotropic volume and vEMDiffuse-a interpolated layers along the z-axis.\\nvEMDiffuse-a training and inference dataset preparation\\nTo prove our concept that vEMDiffuse-a could transfer knowledge learned from the lateral axis (XZ views along the y-axis of anisotropic volumes at the training stage) to the axial axis (XY views along the z-axis of anisotropic volumes at inference stage), we firstly manually downgraded isotropic volumes: Openorganelle Kidney dataset15 and OpenOrganelle Liver isotropic volume44, and trained vEMDiffuse-a on the downgraded anisotropic volume. Given the isotropic volume \\\\(\\\\{{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xy}}}}}}}^{1},{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xy}}}}}}}^{2},\\\\ldots .,{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xy}}}}}}}^{{{{{{\\\\rm{M}}}}}}}\\\\}\\\\), we reduced the axial resolution by removing layers:\\nOf note, in vEMDiffuse-i, we only downgraded the volume in the inference stage while in the training stage, we still used isotropic volumes. Conversely, in vEMDiffuse-a, we downgraded the volume in both the training and inference stage. Then, we chopped along the y-axis to form training data: \\\\(\\\\{{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xz}}}}}}}^{{{{{{\\\\rm{u}}}}}}},{{{{{{\\\\bf{c}}}}}}}_{{{{{{\\\\rm{xz}}}}}}}^{{{{{{\\\\rm{l}}}}}}}\\\\}\\\\) and \\\\(\\\\{{{{{{{\\\\bf{xz}}}}}}}_{0}^{{{{{{\\\\rm{u}}}}}}+1},{{{{{{\\\\bf{xz}}}}}}}_{0}^{{{{{{\\\\rm{u}}}}}}+2},\\\\ldots .,{{{{{{\\\\bf{xz}}}}}}}_{0}^{{{{{{\\\\rm{u}}}}}}+{{{{{\\\\rm{R}}}}}}}\\\\}\\\\) as elucidated above.\\nFor original anisotropic datasets such as the MICrONS multi-area and FANC multi-area datasets, we directly sliced along the y-axis. For both MICrONS multi-area and FANC datasets, we reserved 2048\\u2009√ó\\u20092048\\u2009√ó\\u2009128 subvolume as test datasets (not used in the training stage). In practice, researchers can also use the same anisotropic volume in the training and inference stage.\\nOrganelle segmentation\\nThe organelle segmentation was trained with the isotropic volume and corresponding official segmentation masks provided by Openorganelle15. A 3D U-Net segmentation model was trained on isotropic data. More details of implementation are included in Supplementary Information. Once trained, we applied the model to (1) ground truth isotropic volume, (2) vEMDiffuse generated volume, and (3) interpolated volume. The Intersection over Union (IoU) ratio was calculated against the ground truth mask (isotropic resolution). We applied erosion and dilation post-processing techniques to remove small false positive regions and fill in small hollow holes. As for the anisotropic mask, we directly downsampling the ground truth mask, the same as our protocol for volume downsampling. We utilized Imaris (by Oxford Group) to interpolate the resultant anisotropic mask. We rendered the 3D visualization with Imaris, automatically filtering out some small regions.\\nStatistics and reproducibility\\nFor each denoising and super-resolution experiment, we repeated experiments three times and showed the representative image from 960 images (Figs.\\xa01b, d‚Äìe, 2b, e). Additionally, each vEM reconstruction experiment (including vEMDiffuse-i and vEMDiffuse-a) was independently repeated at least three times for each dataset (Fig.\\xa04b‚Äìd, 4g‚Äìh).\\nReporting summary\\nFurther information on research design is available in the\\xa0Nature Portfolio Reporting Summary linked to this article.\\nData availability\\nDenoising and super-resolution training and test data used in EMDiffuse are available at https://zenodo.org/records/10205819. For vEM, all OpenOrganelle datasets are downloaded from the OpenOrganelle website (https://openorganelle.janelia.org). The Openorganelle Kidney dataset is available at https://doi.org/10.25378/janelia.16913035.v1. The Openorganelle Liver dataset is available at https://doi.org/10.25378/janelia.16913047.v1. The Openorganelle T-Cell dataset is available at https://doi.org/10.6084/m9.figshare.14447541.v1. The EPFL mouse brain dataset is available at https://www.epfl.ch/labs/cvlab/data/data-em/. The MICrONS multi-area dataset can be downloaded from https://www.microns-explorer.org/. The FANC dataset can be downloaded from https://bossdb.org/project/phelps_hildebrand_graham2021. The MANC dataset can be downloaded from https://www.janelia.org/project-team/flyem/manc-connectome.\\xa0Source data are provided with this paper.\\nCode availability\\nThe source codes of EMDiffuse, several representative pre-trained models as well as some example images for testing are publicly accessible via https://github.com/Luchixiang/EMDiffuse.\\nReferences\\nPalade, G. E. & Porter, K. R. Studies on the endoplasmic reticulum: I. Its identification in cells in situ. JEM 100, 641 (1954).\\nArticle\\nCAS\\nGoogle Scholar\\nPalade, G. E. A small particulate component of the cytoplasm. J. Biophys. Biochem. Cytol. 1, 59 (1955).\\nArticle\\nCAS\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nPalade, G. E. A study of fixation for electron microscopy. JEM 95, 285‚Äì298 (1952).\\nArticle\\nCAS\\nGoogle Scholar\\nDenk, W. & Horstmann, H. Serial block-face scanning electron microscopy to reconstruct three-dimensional tissue nanostructure. PLoS Biol. 2, e329 (2004).\\nArticle\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nHayworth, K., Kasthuri, N., Schalek, R. & Lichtman, J. Automating the collection of ultrathin serial sections for large volume TEM reconstructions. Microsc. Microanal. 12, 86‚Äì87 (2006).\\nArticle\\nADS\\nGoogle Scholar\\nHorstmann, H., K√∂rber, C., S√§tzler, K., Aydin, D. & Kuner, T. Serial section scanning electron microscopy (S3EM) on silicon wafers for ultra-structural volume imaging of cells and tissues. PloS ONE 7, e35172 (2012).\\nArticle\\nADS\\nCAS\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nKnott, G., Marchman, H., Wall, D. & Lich, B. Serial section scanning electron microscopy of adult brain tissue using focused ion beam milling. J. Neurosci. 28, 2959‚Äì2964 (2008).\\nArticle\\nCAS\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nLoomba, S. et al. Connectomic comparison of mouse and human cortex. Science 377, eabo0924 (2022).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nGour, A. et al. Postnatal connectomic development of inhibition in mouse barrel cortex. Science 371, eabb4534 (2021).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nZheng, Z. et al. A complete electron microscopy volume of the brain of adult Drosophila melanogaster. Cell 174, 730‚Äì743. e722 (2018).\\nArticle\\nCAS\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nScheffer, L. K. et al. A connectome and analysis of the adult Drosophila central brain. Elife 9, e57443 (2020).\\nArticle\\nCAS\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nMotta, A. et al. Dense connectomic reconstruction in layer 4 of the somatosensory cortex. Science 366, eaay3134 (2019).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nXu, C. S. et al. Enhanced FIB-SEM systems for large-volume 3D imaging. Elife 6, e25916 (2017).\\nArticle\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nM√ºller, A. et al. 3D FIB-SEM reconstruction of microtubule‚Äìorganelle interaction in whole primary mouse Œ≤ cells. JCB 220, e202010039 (2021).\\nXu, C. S. et al. An open-access volume electron microscopy atlas of whole cells and tissues. Nature 599, 147‚Äì151 (2021).\\nArticle\\nADS\\nCAS\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nScheffer, L. K. et al. A connectome and analysis of the adult Drosophila central brain. ELife 9, e57443 (2020).\\nPhelps, J. S. et al. Reconstruction of motor control circuits in adult Drosophila using automated transmission electron microscopy. Cell 184, 759‚Äì774.e718 (2021).\\nArticle\\nCAS\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nFu, S. et al. Field-dependent deep learning enables high-throughput whole-cell 3D super-resolution imaging. Nat. Methods 20, 459‚Äì468 (2023).\\nKrull, A., Buchholz, T.-O. & Jug, F. Noise2void-learning denoising from single noisy images. In Proc. IEEE Conference on Computer Vision and Pattern Recognition 2129‚Äì2137 (IEEE, 2019).\\nOuyang, W., Aristov, A., Lelek, M., Hao, X. & Zimmer, C. Deep learning massively accelerates super-resolution localization microscopy. Nat. Biotechnol. 36, 460‚Äì468 (2018).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nWang, H. et al. Deep learning enables cross-modality super-resolution in fluorescence microscopy. Nat. Methods 16, 103‚Äì110 (2019).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nEde, J. M. Deep learning in electron microscopy. Mach. Learn. Sci. Technol. 2, 011004 (2021).\\nArticle\\nGoogle Scholar\\nFang, L. et al. Deep learning-based point-scanning super-resolution imaging. Nat. Methods 18, 406‚Äì416 (2021).\\nArticle\\nCAS\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nShapson-Coe, A. et al. A petavoxel fragment of human cerebral cortex reconstructed at nanoscale resolution. Science 384, eadk4858 (2024).\\nLi, Y. et al. Incorporating the image formation process into deep learning improves network performance. Nat. Methods 19, 1427‚Äì1437 (2022).\\nQiao, C. et al. Evaluation and development of deep neural networks for image super-resolution in optical microscopy. Nat. Methods 18, 194‚Äì202 (2021).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nQiao, C. et al. Rationalized deep learning super-resolution microscopy for sustained live imaging of rapid subcellular processes. Nat. Biotechnol. 41, 367‚Äì377 (2023).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nWeigert, M. et al. Content-aware image restoration: pushing the limits of fluorescence microscopy. Nat. Methods 15, 1090‚Äì1097 (2018).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nLehtinen, J. et al. Noise2Noise: learning image restoration without clean data. In Proc. of the 35th International Conference on Machine Learning 80, 2965‚Äì2974 (2018).\\nRahaman, N. et al. On the spectral bias of neural networks. In International Conference on Machine Learning 5301‚Äì5310 (Association for Computing Machinery, 2019).\\nZhu, J.-Y., Park, T., Isola, P. & Efros, A. A. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proc. IEEE International Conference on Computer Vision 2223‚Äì2232 (IEEE, 2017).\\nHo, J., Jain, A. & Abbeel, P. Denoising diffusion probabilistic models. Adv. Neural Inf. Process. Syst. 33, 6840‚Äì6851 (MIT Press, 2020).\\nGoogle Scholar\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P. & Ommer, B. High-resolution image synthesis with latent diffusion models. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition 10684‚Äì10695 (IEEE, 2022).\\nSaharia, C. et al. Palette: image-to-image diffusion models. ACM Trans. Graph. 1‚Äì10 (2022).\\nDhariwal, P. & Nichol, A. Diffusion models beat gans on image synthesis. Adv. Neural Inf. Process. Syst. 34, 8780‚Äì8794 (2021).\\nGoogle Scholar\\nLu, C. et al. EMDiffuse: a diffusion-based deep learning method for augmenting ultrastructural imaging and volume electron microscopy. Zenodo https://doi.org/10.5281/zenodo.11066743 (2024).\\nKendall, A. & Gal, Y. What uncertainties do we need in Bayesian deep learning for computer vision? Adv. Neural Inf. Process. Syst. 30, 5574‚Äì5584 (2017).\\nZhang, L., Zhang, L., Mou, X. & Zhang, D. FSIM: a feature similarity index for image quality assessment. IEEE Trans. Image Process. 20, 2378‚Äì2386 (2011).\\nArticle\\nADS\\nMathSciNet\\nPubMed\\nGoogle Scholar\\nZhang, Y. et al. Image super-resolution using very deep residual channel attention networks. In Proc. European Conference on Computer Vision 286‚Äì301 (Springer Science+Business Media, 2018).\\nDescloux, A., Gru√ümayer, K. S. & Radenovic, A. Parameter-free image resolution estimation based on decorrelation analysis. Nat. Methods 16, 918‚Äì924 (2019).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nWang, X. et al. Esrgan: enhanced super-resolution generative adversarial networks. In Proc. European Conference on Computer Vision (ECCV) Workshops (Springer Science+Business Media, 2018).\\nZhang, R., Isola, P., Efros, A. A., Shechtman, E. & Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In Proc. IEEE conference on Computer Vision and Pattern Recognition 586‚Äì595 (IEEE, 2018).\\nHeinrich, L. et al. Whole-cell organelle segmentation in volume electron microscopy. Nature 599, 141‚Äì146 (2021).\\nArticle\\nADS\\nCAS\\nPubMed\\nGoogle Scholar\\nParlakg√ºl, G. et al. Regulation of liver subcellular architecture controls metabolic homeostasis. Nature 603, 736‚Äì742 (2022).\\nArticle\\nADS\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nMcCormick, M., Liu, X., Jomier, J., Marion, C. & Ibanez, L. ITK: enabling reproducible research and open science. Front. Neuroinform. 8, 13 (2014).\\nArticle\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nHeinrich, L., Bogovic, J. A. & Saalfeld, S. Deep learning for isotropic super-resolution from non-isotropic 3D electron microscopy. In Medical Image Computing and Computer-Assisted Intervention 135‚Äì143 (Springer, 2017).\\nRitter, A. T. et al. ESCRT-mediated membrane repair protects tumor-derived cells against T cell attack. Science 376, 377‚Äì382 (2022).\\nArticle\\nADS\\nCAS\\nPubMed\\nGoogle Scholar\\nLucchi, A., Li, Y. & Fua, P. Learning for structured prediction using approximate subgradient descent with working sets. In Proc. IEEE Conference on Computer Vision and Pattern Recognition 1987‚Äì1994 (IEEE, 2013).\\nRonneberger, O., Fischer, P. & Brox, T. U-net: convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention, 234‚Äì241 (Springer, 2015).\\nConsortium, M. et al. Functional connectomics spanning multiple areas of mouse visual cortex. Preprint at bioRxiv https://doi.org/10.1101/2021.07.28.454025 (2021).\\nTakemura, S.-Y. et al. A connectome of the male drosophila ventral nerve cord. Preprint at bioRxiv https://doi.org/10.1101/2023.06.05.543757 (2023).\\nJanuszewski, M. et al. High-precision automated reconstruction of neurons with flood-filling networks. Nat. Methods 15, 605‚Äì610 (2018).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nSutton, M. A., Li, N., Joy, D., Reynolds, A. P. & Li, X. Scanning electron microscopy for quantitative small and large deformation measurements part I: SEM imaging at magnifications from 200 to 10,000. Exp. Mech. 47, 775‚Äì787 (2007).\\nArticle\\nGoogle Scholar\\nBrown, L. G. A survey of image registration techniques. ACM Comput. Surv. 24, 325‚Äì376 (1992).\\nArticle\\nGoogle Scholar\\nRublee, E., Rabaud, V., Konolige, K. & Bradski, G. ORB: an efficient alternative to SIFT or SURF. In Proc. of the IEEE International Conference on Computer Vision 2564‚Äì2571 (IEEE, 2011).\\nTeed, Z. & Deng, J. Raft: recurrent all-pairs field transforms for optical flow. In Proc. European Conference on Computer Vision 402‚Äì419 (Springer Science+Business Media, 2020).\\nPaszke, A. et al. Pytorch: an imperative style, high-performance deep learning library. Adv. Neural Inf. Process. Syst. 32, 8024‚Äì8035 (2019).\\nKingma, D. P. & Ba, J. Adam: a method for stochastic optimization. Preprint at https://arxiv.org/abs/1412.6980 (2014).\\nSaharia, C. et al. Image super-resolution via iterative refinement. IEEE Trans. Pattern Anal. Mach. Intell. 45, 4713‚Äì4726 (2022).\\nGoogle Scholar\\nDownload references\\nAcknowledgements\\nThis work was supported by the Research Grants Council of Hong Kong (17102722, 17202422, 27209621) and the Australian Research Council (LP190100433). The work was conducted in the JC STEM Lab of Molecular Imaging, funded by The Hong Kong Jockey Club Charities Trust. We also thank Dr. Jing Guo for her assistance in image processing.\\nAuthor information\\nAuthors and Affiliations\\nDepartment of Chemistry, The University of Hong Kong, Hong Kong, China\\nChixiang Lu,\\xa0Kai Chen,\\xa0Heng Qiu,\\xa0Gu Chen\\xa0&\\xa0Haibo Jiang\\nSchool of Molecular Sciences, The University of Western Australia, Perth, WA, Australia\\nKai Chen\\xa0&\\xa0Xiaojun Chen\\nDepartment of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong, China\\nXiaojuan Qi\\nYou can also search for this author in\\nPubMed\\xa0Google Scholar\\nYou can also search for this author in\\nPubMed\\xa0Google Scholar\\nYou can also search for this author in\\nPubMed\\xa0Google Scholar\\nYou can also search for this author in\\nPubMed\\xa0Google Scholar\\nYou can also search for this author in\\nPubMed\\xa0Google Scholar\\nYou can also search for this author in\\nPubMed\\xa0Google Scholar\\nYou can also search for this author in\\nPubMed\\xa0Google Scholar\\nContributions\\nH.J., X.Q., and C.L. designed the experiments and wrote the paper. C.L., K.C., H.Q., X.C., and G.C. performed, collected, and assembled the experiments. H.J. and X.Q. secured funding. All have commented on and edited the manuscript.\\nCorresponding authors\\nCorrespondence to\\nXiaojuan Qi or Haibo Jiang.\\nEthics declarations\\nCompeting interests\\nThe authors have declared that no conflict of interest exists.\\nPeer review\\nPeer review information\\nNature Communications thanks Yoshiyuki Kubota, Constantin Pape, and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. A peer review file is available.\\nAdditional information\\nPublisher‚Äôs note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\\nSupplementary information\\nSupplementary Information\\nPeer Review File\\nDescription of Additional Supplementary Files\\nSupplementary Movie 1\\nSupplementary Movie 2\\nSupplementary Movie 3\\nSupplementary Movie 4\\nSupplementary Movie 5\\nSupplementary Movie 6\\nSupplementary Movie 7\\nSupplementary Movie 8\\nSupplementary Movie 9\\nSupplementary Movie 10\\nSupplementary Movie 11\\nSupplementary Movie 12\\nSupplementary Movie 13\\nSupplementary Movie 14\\nSupplementary Movie 15\\nSupplementary Movie 16\\nReporting Summary\\nSource data\\nSource Data\\nRights and permissions\\nOpen Access\\nThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article‚Äôs Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article‚Äôs Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nReprints and permissions\\nAbout this article\\nCite this article\\nLu, C., Chen, K., Qiu, H. et al. Diffusion-based deep learning method for augmenting ultrastructural imaging and volume electron microscopy.\\nNat Commun 15, 4677 (2024). https://doi.org/10.1038/s41467-024-49125-z\\nDownload citation\\nReceived: 18 August 2023\\nAccepted: 20 May 2024\\nPublished: 01 June 2024\\nDOI: https://doi.org/10.1038/s41467-024-49125-z\\nShare this article\\nAnyone you share the following link with will be able to read this content:\\nSorry, a shareable link is not currently available for this article.\\nProvided by the Springer Nature SharedIt content-sharing initiative\\nComments\\nBy submitting a comment you agree to abide by our Terms and Community Guidelines. If you find something abusive or that does not comply with our terms or guidelines please flag it as inappropriate.\\nAdvertisement\\nExplore content\\nAbout the journal\\nPublish with us\\nSearch\\nQuick links\\nNature Communications (Nat Commun)\\nISSN 2041-1723 (online)\\nnature.com sitemap\\nAbout Nature Portfolio\\nDiscover content\\nPublishing policies\\nAuthor & Researcher services\\nLibraries & institutions\\nAdvertising & partnerships\\nProfessional development\\nRegional websites\\n¬© 2024 Springer Nature Limited\\nSign up for the Nature Briefing: AI and Robotics newsletter ‚Äî what matters in AI and robotics research, free to your inbox weekly.', 'time': None}, {'url': 'https://arxiv.org/pdf/2209.00796v9', 'raw_content': 'Diffusion models have emerged as a powerful new family of deep generative\\nmodels with record-breaking performance in many applications, including image\\nsynthesis, video generation, and molecule design. In this survey, we provide an\\noverview of the rapidly expanding body of work on diffusion models,\\ncategorizing the research into three key areas: efficient sampling, improved\\nlikelihood estimation, and handling data with special structures. We also\\ndiscuss the potential for combining diffusion models with other generative\\nmodels for enhanced results. We further review the wide-ranging applications of\\ndiffusion models in fields spanning from computer vision, natural language\\nprocessing, temporal data modeling, to interdisciplinary applications in other\\nscientific disciplines. This survey aims to provide a contextualized, in-depth\\nlook at the state of diffusion models, identifying the key areas of focus and\\npointing to potential areas for further exploration. Github:\\nhttps://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.', 'time': datetime.date(2022, 10, 24)}, {'url': 'https://ieeexplore.ieee.org/document/10505362', 'raw_content': \"IEEE Account\\nPurchase Details\\nProfile Information\\nNeed Help?\\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.¬© Copyright 2024 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.\", 'time': None}])\n",
            "üìÉ Source: https://arxiv.org/pdf/2209.00796v9\n",
            "Title: \n",
            "Published_time: 2022-10-24\n",
            "Content: Diffusion models have emerged as a powerful new family of deep generative\n",
            "models with record-breaking performance in many applications, including image\n",
            "synthesis, video generation, and molecule design. In this survey, we provide an\n",
            "overview of the rapidly expanding body of work on diffusion models,\n",
            "categorizing the research into three key areas: efficient sampling, improved\n",
            "likelihood estimation, and handling data with special structures. We also\n",
            "discuss the potential for combining diffusion models with other generative\n",
            "models for enhanced results. We further review the wide-ranging applications of\n",
            "diffusion models in fields spanning from computer vision, natural language\n",
            "processing, temporal data modeling, to interdisciplinary applications in other\n",
            "scientific disciplines. This survey aims to provide a contextualized, in-depth\n",
            "look at the state of diffusion models, identifying the key areas of focus and\n",
            "pointing to potential areas for further exploration. Github:\n",
            "\n",
            "Source: https://www.nature.com/articles/s41467-024-49125-z\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: Ho, J., Jain, A. & Abbeel, P. Denoising diffusion probabilistic models. Adv. Neural Inf. Process. Syst. 33, 6840‚Äì6851 (MIT Press, 2020).\n",
            "Google Scholar\n",
            "Rombach, R., Blattmann, A., Lorenz, D., Esser, P. & Ommer, B. High-resolution image synthesis with latent diffusion models. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition 10684‚Äì10695 (IEEE, 2022).\n",
            "Saharia, C. et al. Palette: image-to-image diffusion models. ACM Trans. Graph. 1‚Äì10 (2022).\n",
            "Dhariwal, P. & Nichol, A. Diffusion models beat gans on image synthesis. Adv. Neural Inf. Process. Syst. 34, 8780‚Äì8794 (2021).\n",
            "Google Scholar\n",
            "Lu, C. et al. EMDiffuse: a diffusion-based deep learning method for augmenting ultrastructural imaging and volume electron microscopy. Zenodo https://doi.org/10.5281/zenodo.11066743 (2024).\n",
            "Kendall, A. & Gal, Y. What uncertainties do we need in Bayesian deep learning for computer vision? Adv. Neural Inf. Process. Syst. 30, 5574‚Äì5584 (2017).\n",
            "\n",
            "Source: https://www.nature.com/articles/s41467-024-49125-z\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: Article\n",
            "CAS\n",
            "PubMed\n",
            "Google Scholar\n",
            "Qiao, C. et al. Rationalized deep learning super-resolution microscopy for sustained live imaging of rapid subcellular processes. Nat. Biotechnol. 41, 367‚Äì377 (2023).\n",
            "Article\n",
            "CAS\n",
            "PubMed\n",
            "Google Scholar\n",
            "Weigert, M. et al. Content-aware image restoration: pushing the limits of fluorescence microscopy. Nat. Methods 15, 1090‚Äì1097 (2018).\n",
            "Article\n",
            "CAS\n",
            "PubMed\n",
            "Google Scholar\n",
            "Lehtinen, J. et al. Noise2Noise: learning image restoration without clean data. In Proc. of the 35th International Conference on Machine Learning 80, 2965‚Äì2974 (2018).\n",
            "Rahaman, N. et al. On the spectral bias of neural networks. In International Conference on Machine Learning 5301‚Äì5310 (Association for Computing Machinery, 2019).\n",
            "Zhu, J.-Y., Park, T., Isola, P. & Efros, A. A. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proc. IEEE International Conference on Computer Vision 2223‚Äì2232 (IEEE, 2017).\n",
            "\n",
            "Source: https://www.nature.com/articles/s41467-024-49125-z\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: In this study, we propose EMDiffuse, a diffusion model-based package for EM applications, aiming to enhance EM ultrastructural imaging and expand the realm of vEM capabilities. Diffusion models have demonstrated superiority over regression-based models32,33,34 and exhibit greater stability in training than GAN-based models35 regarding image generation and restoration tasks due to their distinctive diffusion-based training and inference schemes. Diffusion models also generate outputs with high-resolution details, which is critical for imaging intricate nanoscale cellular structures with EM. Here, we adopted the diffusion model for EM applications and developed EMDiffuse-n for EM denoising, EMDiffuse-r for EM super-resolution, and vEMDiffuse-i and vEMDiffuse-a for generating isotropic resolution data from anisotropic volumes for vEM. Moreover, we demonstrate the self-assessment capability of EMDiffuse, taking advantage of the inherent ability of diffusion models to generate an arbitrary\n",
            "\n",
            "Source: https://www.nature.com/articles/s41467-024-49125-z\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: Zhang, L., Zhang, L., Mou, X. & Zhang, D. FSIM: a feature similarity index for image quality assessment. IEEE Trans. Image Process. 20, 2378‚Äì2386 (2011).\n",
            "Article\n",
            "ADS\n",
            "MathSciNet\n",
            "PubMed\n",
            "Google Scholar\n",
            "Zhang, Y. et al. Image super-resolution using very deep residual channel attention networks. In Proc. European Conference on Computer Vision 286‚Äì301 (Springer Science+Business Media, 2018).\n",
            "Descloux, A., Gru√ümayer, K. S. & Radenovic, A. Parameter-free image resolution estimation based on decorrelation analysis. Nat. Methods 16, 918‚Äì924 (2019).\n",
            "Article\n",
            "CAS\n",
            "PubMed\n",
            "Google Scholar\n",
            "Wang, X. et al. Esrgan: enhanced super-resolution generative adversarial networks. In Proc. European Conference on Computer Vision (ECCV) Workshops (Springer Science+Business Media, 2018).\n",
            "Zhang, R., Isola, P., Efros, A. A., Shechtman, E. & Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In Proc. IEEE conference on Computer Vision and Pattern Recognition 586‚Äì595 (IEEE, 2018).\n",
            "\n",
            "Source: https://www.nature.com/articles/s41467-024-49125-z\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: Movie¬†3). Moreover, we also showed that the performance of the pre-trained EMDiffuse-n model could be easily improved by fine-tuning the decoder with only a single data pair of images of 3 megapixels from new domains (Supplementary Fig.¬†7). After fine-tuning, the predictions from all four datasets showed low uncertainty values and high reliability (Fig.¬†1e). In contrast, hundreds of pairs of training data were required to train the model from scratch on a new domain. This opens the possibility of applying EMDiffuse-n in various EM application scenarios.\n",
            "\n",
            "Source: https://www.nature.com/articles/s41467-024-49125-z\n",
            "Title: \n",
            "Published_time: None\n",
            "Content: Article\n",
            "CAS\n",
            "PubMed\n",
            "Google Scholar\n",
            "Wang, H. et al. Deep learning enables cross-modality super-resolution in fluorescence microscopy. Nat. Methods 16, 103‚Äì110 (2019).\n",
            "Article\n",
            "CAS\n",
            "PubMed\n",
            "Google Scholar\n",
            "Ede, J. M. Deep learning in electron microscopy. Mach. Learn. Sci. Technol. 2, 011004 (2021).\n",
            "Article\n",
            "Google Scholar\n",
            "Fang, L. et al. Deep learning-based point-scanning super-resolution imaging. Nat. Methods 18, 406‚Äì416 (2021).\n",
            "Article\n",
            "CAS\n",
            "PubMed\n",
            "PubMed Central\n",
            "Google Scholar\n",
            "Shapson-Coe, A. et al. A petavoxel fragment of human cerebral cortex reconstructed at nanoscale resolution. Science 384, eadk4858 (2024).\n",
            "Li, Y. et al. Incorporating the image formation process into deep learning improves network performance. Nat. Methods 19, 1427‚Äì1437 (2022).\n",
            "Qiao, C. et al. Evaluation and development of deep neural networks for image super-resolution in optical microscopy. Nat. Methods 18, 194‚Äì202 (2021).\n",
            "Article\n",
            "CAS\n",
            "PubMed\n",
            "Google Scholar\n",
            "\n",
            "Source: https://arxiv.org/pdf/2209.00796v9\n",
            "Title: \n",
            "Published_time: 2022-10-24\n",
            "Content: pointing to potential areas for further exploration. Github:\n",
            "https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.\n",
            "\n",
            "Finalized research step.\n",
            "üí∏ Total Research Costs: $0.005154040000000001\n",
            "[\"Source: https://ar5iv.labs.arxiv.org/html/2209.00796\\nTitle: \\nPublished_time: 2024-02-06\\nContent: Diffusion models have emerged as a powerful new family of deep generative\\nmodels with record-breaking performance in many applications, including image\\nsynthesis, video generation, and molecule design. In this survey, we provide an\\noverview of the rapidly expanding body of work on diffusion models,\\ncategorizing the research into three key areas: efficient sampling, improved\\nlikelihood estimation, and handling data with special structures. We also\\ndiscuss the potential for combining diffusion models with other generative\\nmodels for enhanced results. We further review the wide-ranging applications of\\ndiffusion models in fields spanning from computer vision, natural language\\ngeneration, temporal data modeling, to interdisciplinary applications in other\\nscientific disciplines. This survey aims to provide a contextualized, in-depth\\nlook at the state of diffusion models, identifying the key areas of focus and\\npointing to potential areas for further exploration. Github:\\n\\nSource: https://arxiv.org/abs/2308.13142\\nTitle: \\nPublished_time: 2023-08-25\\nContent: Recently, there has been significant progress in the development of large\\nmodels. Following the success of ChatGPT, numerous language models have been\\nintroduced, demonstrating remarkable performance. Similar advancements have\\nalso been observed in image generation models, such as Google's Imagen model,\\nOpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive\\ncapabilities in generating images. However, similar to large language models,\\nthese models still encounter unresolved challenges. Fortunately, the\\navailability of open-source stable diffusion models and their underlying\\nmathematical principles has enabled the academic community to extensively\\nanalyze the performance of current image generation models and make\\nimprovements based on this stable diffusion framework. This survey aims to\\nexamine the existing issues and the current solutions pertaining to image\\ngeneration models.\\n\\nSource: https://ar5iv.labs.arxiv.org/html/2308.13142\\nTitle: \\nPublished_time: 2023-08-25\\nContent: Recently, there has been significant progress in the development of large\\nmodels. Following the success of ChatGPT, numerous language models have been\\nintroduced, demonstrating remarkable performance. Similar advancements have\\nalso been observed in image generation models, such as Google's Imagen model,\\nOpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive\\ncapabilities in generating images. However, similar to large language models,\\nthese models still encounter unresolved challenges. Fortunately, the\\navailability of open-source stable diffusion models and their underlying\\nmathematical principles has enabled the academic community to extensively\\nanalyze the performance of current image generation models and make\\nimprovements based on this stable diffusion framework. This survey aims to\\nexamine the existing issues and the current solutions pertaining to image\\ngeneration models.\\n\\nSource: https://arxiv.org/pdf/2308.13142v1\\nTitle: \\nPublished_time: 2023-08-25\\nContent: Recently, there has been significant progress in the development of large\\nmodels. Following the success of ChatGPT, numerous language models have been\\nintroduced, demonstrating remarkable performance. Similar advancements have\\nalso been observed in image generation models, such as Google's Imagen model,\\nOpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive\\ncapabilities in generating images. However, similar to large language models,\\nthese models still encounter unresolved challenges. Fortunately, the\\navailability of open-source stable diffusion models and their underlying\\nmathematical principles has enabled the academic community to extensively\\nanalyze the performance of current image generation models and make\\nimprovements based on this stable diffusion framework. This survey aims to\\nexamine the existing issues and the current solutions pertaining to image\\ngeneration models.\\n\\nSource: https://ar5iv.labs.arxiv.org/html/2303.07909\\nTitle: \\nPublished_time: 2023-04-02\\nContent: This survey reviews text-to-image diffusion models in the context that\\ndiffusion models have emerged to be popular for a wide range of generative\\ntasks. As a self-contained work, this survey starts with a brief introduction\\nof how a basic diffusion model works for image synthesis, followed by how\\ncondition or guidance improves learning. Based on that, we present a review of\\nstate-of-the-art methods on text-conditioned image synthesis, i.e.,\\ntext-to-image. We further summarize applications beyond text-to-image\\ngeneration: text-guided creative generation and text-guided image editing.\\nBeyond the progress made so far, we discuss existing challenges and promising\\nfuture directions.\\n\\nSource: https://ar5iv.labs.arxiv.org/html/2209.00796\\nTitle: \\nPublished_time: 2024-02-06\\nContent: pointing to potential areas for further exploration. Github:\\nhttps://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.\\n\", \"Source: https://news.mit.edu/2024/ai-generates-high-quality-images-30-times-faster-single-step-0321\\nTitle: \\nPublished_time: None\\nContent: ‚ÄúOur work is a novel method that accelerates current diffusion models such as Stable Diffusion and DALLE-3 by 30 times,‚Äù says Tianwei Yin, an MIT PhD student in electrical engineering and computer science, CSAIL affiliate, and the lead researcher on the DMD framework. ‚ÄúThis advancement not only significantly reduces computational time but also retains, if not surpasses, the quality of the generated visual content. Theoretically, the approach marries the principles of generative adversarial networks (GANs) with those of diffusion models, achieving visual content generation in a single step ‚Äî a stark contrast to the hundred steps of iterative refinement required by current diffusion models. It could potentially be a new generative modeling method that excels in speed and quality.‚Äù\\nThis single-step diffusion model could enhance design tools, enabling quicker content creation and potentially supporting advancements in drug discovery and 3D modeling, where promptness and efficacy are key.\\n\\nSource: https://news.mit.edu/2024/ai-generates-high-quality-images-30-times-faster-single-step-0321\\nTitle: \\nPublished_time: None\\nContent: Additionally, the performance of the DMD-generated images is intrinsically linked to the capabilities of the teacher model used during the distillation process. In the current form, which uses Stable Diffusion v1.5 as the teacher model, the student inherits limitations such as rendering detailed depictions of text and small faces, suggesting that DMD-generated images could be further enhanced by more advanced teacher models.\\n‚ÄúDecreasing the number of iterations has been the Holy Grail in diffusion models since their inception,‚Äù says Fredo Durand, MIT professor of electrical engineering and computer science, CSAIL principal investigator, and a lead author on the paper. ‚ÄúWe are very excited to finally enable single-step image generation, which will dramatically reduce compute costs and accelerate the process.‚Äù\\n\\nSource: https://arxiv.org/abs/2406.02347\\nTitle: \\nPublished_time: 2024-06-05\\nContent: In this paper, we propose an efficient, fast, and versatile distillation\\nmethod to accelerate the generation of pre-trained diffusion models: Flash\\nDiffusion. The method reaches state-of-the-art performances in terms of FID and\\nCLIP-Score for few steps image generation on the COCO2014 and COCO2017\\ndatasets, while requiring only several GPU hours of training and fewer\\ntrainable parameters than existing methods. In addition to its efficiency, the\\nversatility of the method is also exposed across several tasks such as\\ntext-to-image, inpainting, face-swapping, super-resolution and using different\\nbackbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\\\\alpha$),\\nas well as adapters. In all cases, the method allowed to reduce drastically the\\nnumber of sampling steps while maintaining very high-quality image generation.\\nThe official implementation is available at\\nhttps://github.com/gojasper/flash-diffusion.\\n\\nSource: https://scitechdaily.com/mits-new-generative-ai-outperforms-diffusion-models-in-image-generation/\\nTitle: \\nPublished_time: None\\nContent: MIT‚Äôs New Generative AI Outperforms Diffusion Models in Image Generation\\nBy Rachel Gordon, MIT CSAIL October 14, 2023\\nMIT‚Äôs CSAIL introduces the PFGM++, an AI model combining diffusion and Poisson Flow principles. It offers superior image generation by replicating electric field behaviors, representing a leap in generative AI.\\nInspired by physics, a new generative model PFGM++ outperforms diffusion models in image generation.\\nGenerative AI, which is currently riding a crest of popular discourse, promises a world where the simple transforms into the complex ‚Äî where a simple distribution evolves into intricate patterns of images, sounds, or text, rendering the artificial startlingly real.\\n\\nSource: https://scitechdaily.com/mits-new-generative-ai-outperforms-diffusion-models-in-image-generation/\\nTitle: \\nPublished_time: None\\nContent: Industry Feedback\\n‚ÄúDiffusion models have become a critical driving force behind the revolution in generative AI,‚Äù says Yang Song, research scientist at OpenAI. ‚ÄúPFGM++ presents a powerful generalization of diffusion models, allowing users to generate higher-quality images by improving the robustness of image generation against perturbations and learning errors. Furthermore, PFGM++ uncovers a surprising connection between electrostatics and diffusion models, providing new theoretical insights into diffusion model research.‚Äù\\n‚ÄúPoisson Flow Generative Models do not only rely on an elegant physics-inspired formulation based on electrostatics, but they also offer state-of-the-art generative modeling performance in practice,‚Äù says NVIDIA Senior Research Scientist Karsten Kreis, who was not involved in the work.\\n\\nSource: https://news.mit.edu/2024/ai-generates-high-quality-images-30-times-faster-single-step-0321\\nTitle: \\nPublished_time: None\\nContent: When put to the test against the usual methods, using a wide range of benchmarks, DMD showed consistent performance. On the popular benchmark of generating images based on specific classes on ImageNet, DMD is the first one-step diffusion technique that churns out pictures pretty much on par with those from the original, more complex models, rocking a super-close Fr√©chet inception distance (FID) score of just 0.3, which is impressive, since FID is all about judging the quality and diversity of generated images. Furthermore, DMD excels in industrial-scale text-to-image generation and achieves state-of-the-art one-step generation performance. There's still a slight quality gap when tackling trickier text-to-image applications, suggesting there's a bit of room for improvement down the line.\\n\\nSource: https://news.mit.edu/2024/ai-generates-high-quality-images-30-times-faster-single-step-0321\\nTitle: \\nPublished_time: None\\nContent: Suggestions or feedback?\\nMIT News | Massachusetts Institute of Technology\\nBrowse By\\nTopics\\nDepartments\\nCenters, Labs, & Programs\\nSchools\\nBreadcrumb\\nAI generates high-quality images 30 times faster in a single step\\nPress Contact:\\nPrevious image\\nNext image\\nIn our current age of artificial intelligence, computers can generate their own ‚Äúart‚Äù by way of diffusion models, iteratively adding structure to a noisy initial state until a clear image or video emerges. Diffusion models have suddenly grabbed a seat at everyone‚Äôs table: Enter a few words and experience instantaneous, dopamine-spiking dreamscapes at the intersection of reality and fantasy. Behind the scenes, it involves a complex, time-intensive process requiring numerous iterations for the algorithm to perfect the image.\\n\\nSource: https://news.mit.edu/2024/ai-generates-high-quality-images-30-times-faster-single-step-0321\\nTitle: \\nPublished_time: None\\nContent: ‚ÄúFinally, a paper that successfully combines the versatility and high visual quality of diffusion models with the real-time performance of GANs,‚Äù says Alexei Efros, a professor of electrical engineering and computer science at the University of California at Berkeley who was not involved in this study. ‚ÄúI expect this work to open up fantastic possibilities for high-quality real-time visual editing.‚Äù\\n\", \"Source: https://arxiv.org/pdf/2403.06807\\nTitle: \\nPublished_time: 2024-06-03\\nContent: FID on Imagenet128 in 8 steps with consistency distillation, using simple\\nlosses without adversarial training. We also show that our method scales to a\\ntext-to-image diffusion model, generating samples that are close to the quality\\nof the original model.\\n\\nSource: https://community.openai.com/t/seeking-ideas-for-text-to-image-generation/806633\\nTitle: \\nPublished_time: None\\nContent: Thank you in advance for your help!\\nThere are two kinds of image models: GAN and Diffusion. Currently, most people are using diffusion for their models, such as DALL-E and Stable Diffusion, as they are the best. An idea would be to do more research on GAN, as that has significantly slowed down. Another idea would be to create image upscalers using GANs, as I don‚Äôt believe I have ever seen a GAN upscaler before, as they are mostly diffusion models. I made a GAN a bit ago and some code and results can be found here: GitHub - grandell1234/S.C.O.R.P: Text-To-Image GAN Model\\nDoes creating image upscalers using GANs, means inputting a low quality image and generating a high quality image from that. like the Super-Resolution Generative Adversarial Networks (SRGANs). Can you explain\\na bit more about it.\\nYeah, or taking it from like 512x512 to 1280x1280 adding pixels where required.\\n\\nSource: https://arxiv.org/pdf/2406.03184\\nTitle: \\nPublished_time: 2024-06-05\\nContent: Existing single image-to-3D creation methods typically involve a two-stage\\nprocess, first generating multi-view images, and then using these images for 3D\\nreconstruction. However, training these two stages separately leads to\\nsignificant data bias in the inference phase, thus affecting the quality of\\nreconstructed results. We introduce a unified 3D generation framework, named\\nOuroboros3D, which integrates diffusion-based multi-view image generation and\\n3D reconstruction into a recursive diffusion process. In our framework, these\\ntwo modules are jointly trained through a self-conditioning mechanism, allowing\\nthem to adapt to each other's characteristics for robust inference. During the\\nmulti-view denoising process, the multi-view diffusion model uses the 3D-aware\\nmaps rendered by the reconstruction module at the previous timestep as\\nadditional conditions. The recursive diffusion framework with 3D-aware feedback\\n\\nSource: https://community.openai.com/t/seeking-ideas-for-text-to-image-generation/806633\\nTitle: \\nPublished_time: None\\nContent: Seeking ideas for text to image generation\\nHello everyone,\\nI am a university student currently working on my final year research project, and I am looking for ideas related to text-to-image generation. My goal is to identify a meaningful research gap in this field that I can explore.\\nSo far, I have reviewed several existing models and techniques, such as DALL-E and other GAN-based methods, but I am struggling to pinpoint a specific area that has not been extensively covered. I am particularly interested in topics that could contribute to improving image quality, semantic coherence, or other aspects of text-to-image models.\\nCould anyone recommend some potential research areas or gaps in the current literature that I could investigate? Any suggestions, papers, or resources would be greatly appreciated.\\nThank you in advance for your help!\\n\\nSource: https://arxiv.org/pdf/2406.03184\\nTitle: \\nPublished_time: 2024-06-05\\nContent: additional conditions. The recursive diffusion framework with 3D-aware feedback\\nunites the entire process and improves geometric consistency.Experiments show\\nthat our framework outperforms separation of these two stages and existing\\nmethods that combine them at the inference phase. Project page:\\nhttps://costwen.github.io/Ouroboros3D/\\n\\nSource: https://arxiv.org/pdf/2403.06807\\nTitle: \\nPublished_time: 2024-06-03\\nContent: Diffusion models are relatively easy to train but require many steps to\\ngenerate samples. Consistency models are far more difficult to train, but\\ngenerate samples in a single step.\\n  In this paper we propose Multistep Consistency Models: A unification between\\nConsistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that\\ncan interpolate between a consistency model and a diffusion model: a trade-off\\nbetween sampling speed and sampling quality. Specifically, a 1-step consistency\\nmodel is a conventional consistency model whereas a $\\\\infty$-step consistency\\nmodel is a diffusion model.\\n  Multistep Consistency Models work really well in practice. By increasing the\\nsample budget from a single step to 2-8 steps, we can train models more easily\\nthat generate higher quality samples, while retaining much of the sampling\\nspeed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1\\nFID on Imagenet128 in 8 steps with consistency distillation, using simple\\n\\nSource: https://community.openai.com/t/seeking-ideas-for-text-to-image-generation/806633\\nTitle: \\nPublished_time: None\\nContent: Yeah, or taking it from like 512x512 to 1280x1280 adding pixels where required.\\nCurrently I ran into a problem where Dalle can‚Äôt divide the rendered image into exact spaces, for example a rectangle into 8 exact boxes, then making an image for every box. The problem is I‚Äôve seen it accomplished by one GPT. The other obvious issue is rendering text or counting. I think Dalle should add a layer of text object on top of the Dalle image then merge them prior to output. If the Coordinates can be matched then we resolve this issue completely by merging text logic with the image logic, then merging them prior to output. So an instruction to create art at X, Y coordinates of an image. Once this is mastered, commands can place text.\\nEven HTML can accomplish this simple concept of a background image under text. Dalle needs to understand its canvas space. To take a command like:\\nCreate 8 boxes with random art numbered 1-8, center text.\\n\\nSource: https://community.openai.com/t/seeking-ideas-for-text-to-image-generation/806633\\nTitle: \\nPublished_time: None\\nContent: Create 8 boxes with random art numbered 1-8, center text.\\nWe can‚Äôt even get Dalle to return 8 exact sized boxes of art yet.\\n‚Ä¶\\nRelated Topics\\nPowered by Discourse, best viewed with JavaScript enabled\\n\", 'Source: https://arxiv.org/pdf/2209.00796v9\\nTitle: \\nPublished_time: 2022-10-24\\nContent: Diffusion models have emerged as a powerful new family of deep generative\\nmodels with record-breaking performance in many applications, including image\\nsynthesis, video generation, and molecule design. In this survey, we provide an\\noverview of the rapidly expanding body of work on diffusion models,\\ncategorizing the research into three key areas: efficient sampling, improved\\nlikelihood estimation, and handling data with special structures. We also\\ndiscuss the potential for combining diffusion models with other generative\\nmodels for enhanced results. We further review the wide-ranging applications of\\ndiffusion models in fields spanning from computer vision, natural language\\nprocessing, temporal data modeling, to interdisciplinary applications in other\\nscientific disciplines. This survey aims to provide a contextualized, in-depth\\nlook at the state of diffusion models, identifying the key areas of focus and\\npointing to potential areas for further exploration. Github:\\n\\nSource: https://www.nature.com/articles/s41467-024-49125-z\\nTitle: \\nPublished_time: None\\nContent: Ho, J., Jain, A. & Abbeel, P. Denoising diffusion probabilistic models. Adv. Neural Inf. Process. Syst. 33, 6840‚Äì6851 (MIT Press, 2020).\\nGoogle Scholar\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P. & Ommer, B. High-resolution image synthesis with latent diffusion models. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition 10684‚Äì10695 (IEEE, 2022).\\nSaharia, C. et al. Palette: image-to-image diffusion models. ACM Trans. Graph. 1‚Äì10 (2022).\\nDhariwal, P. & Nichol, A. Diffusion models beat gans on image synthesis. Adv. Neural Inf. Process. Syst. 34, 8780‚Äì8794 (2021).\\nGoogle Scholar\\nLu, C. et al. EMDiffuse: a diffusion-based deep learning method for augmenting ultrastructural imaging and volume electron microscopy. Zenodo https://doi.org/10.5281/zenodo.11066743 (2024).\\nKendall, A. & Gal, Y. What uncertainties do we need in Bayesian deep learning for computer vision? Adv. Neural Inf. Process. Syst. 30, 5574‚Äì5584 (2017).\\n\\nSource: https://www.nature.com/articles/s41467-024-49125-z\\nTitle: \\nPublished_time: None\\nContent: Article\\nCAS\\nPubMed\\nGoogle Scholar\\nQiao, C. et al. Rationalized deep learning super-resolution microscopy for sustained live imaging of rapid subcellular processes. Nat. Biotechnol. 41, 367‚Äì377 (2023).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nWeigert, M. et al. Content-aware image restoration: pushing the limits of fluorescence microscopy. Nat. Methods 15, 1090‚Äì1097 (2018).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nLehtinen, J. et al. Noise2Noise: learning image restoration without clean data. In Proc. of the 35th International Conference on Machine Learning 80, 2965‚Äì2974 (2018).\\nRahaman, N. et al. On the spectral bias of neural networks. In International Conference on Machine Learning 5301‚Äì5310 (Association for Computing Machinery, 2019).\\nZhu, J.-Y., Park, T., Isola, P. & Efros, A. A. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proc. IEEE International Conference on Computer Vision 2223‚Äì2232 (IEEE, 2017).\\n\\nSource: https://www.nature.com/articles/s41467-024-49125-z\\nTitle: \\nPublished_time: None\\nContent: In this study, we propose EMDiffuse, a diffusion model-based package for EM applications, aiming to enhance EM ultrastructural imaging and expand the realm of vEM capabilities. Diffusion models have demonstrated superiority over regression-based models32,33,34 and exhibit greater stability in training than GAN-based models35 regarding image generation and restoration tasks due to their distinctive diffusion-based training and inference schemes. Diffusion models also generate outputs with high-resolution details, which is critical for imaging intricate nanoscale cellular structures with EM. Here, we adopted the diffusion model for EM applications and developed EMDiffuse-n for EM denoising, EMDiffuse-r for EM super-resolution, and vEMDiffuse-i and vEMDiffuse-a for generating isotropic resolution data from anisotropic volumes for vEM. Moreover, we demonstrate the self-assessment capability of EMDiffuse, taking advantage of the inherent ability of diffusion models to generate an arbitrary\\n\\nSource: https://www.nature.com/articles/s41467-024-49125-z\\nTitle: \\nPublished_time: None\\nContent: Zhang, L., Zhang, L., Mou, X. & Zhang, D. FSIM: a feature similarity index for image quality assessment. IEEE Trans. Image Process. 20, 2378‚Äì2386 (2011).\\nArticle\\nADS\\nMathSciNet\\nPubMed\\nGoogle Scholar\\nZhang, Y. et al. Image super-resolution using very deep residual channel attention networks. In Proc. European Conference on Computer Vision 286‚Äì301 (Springer Science+Business Media, 2018).\\nDescloux, A., Gru√ümayer, K. S. & Radenovic, A. Parameter-free image resolution estimation based on decorrelation analysis. Nat. Methods 16, 918‚Äì924 (2019).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nWang, X. et al. Esrgan: enhanced super-resolution generative adversarial networks. In Proc. European Conference on Computer Vision (ECCV) Workshops (Springer Science+Business Media, 2018).\\nZhang, R., Isola, P., Efros, A. A., Shechtman, E. & Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In Proc. IEEE conference on Computer Vision and Pattern Recognition 586‚Äì595 (IEEE, 2018).\\n\\nSource: https://www.nature.com/articles/s41467-024-49125-z\\nTitle: \\nPublished_time: None\\nContent: Movie\\xa03). Moreover, we also showed that the performance of the pre-trained EMDiffuse-n model could be easily improved by fine-tuning the decoder with only a single data pair of images of 3 megapixels from new domains (Supplementary Fig.\\xa07). After fine-tuning, the predictions from all four datasets showed low uncertainty values and high reliability (Fig.\\xa01e). In contrast, hundreds of pairs of training data were required to train the model from scratch on a new domain. This opens the possibility of applying EMDiffuse-n in various EM application scenarios.\\n\\nSource: https://www.nature.com/articles/s41467-024-49125-z\\nTitle: \\nPublished_time: None\\nContent: Article\\nCAS\\nPubMed\\nGoogle Scholar\\nWang, H. et al. Deep learning enables cross-modality super-resolution in fluorescence microscopy. Nat. Methods 16, 103‚Äì110 (2019).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\nEde, J. M. Deep learning in electron microscopy. Mach. Learn. Sci. Technol. 2, 011004 (2021).\\nArticle\\nGoogle Scholar\\nFang, L. et al. Deep learning-based point-scanning super-resolution imaging. Nat. Methods 18, 406‚Äì416 (2021).\\nArticle\\nCAS\\nPubMed\\nPubMed Central\\nGoogle Scholar\\nShapson-Coe, A. et al. A petavoxel fragment of human cerebral cortex reconstructed at nanoscale resolution. Science 384, eadk4858 (2024).\\nLi, Y. et al. Incorporating the image formation process into deep learning improves network performance. Nat. Methods 19, 1427‚Äì1437 (2022).\\nQiao, C. et al. Evaluation and development of deep neural networks for image super-resolution in optical microscopy. Nat. Methods 18, 194‚Äì202 (2021).\\nArticle\\nCAS\\nPubMed\\nGoogle Scholar\\n\\nSource: https://arxiv.org/pdf/2209.00796v9\\nTitle: \\nPublished_time: 2022-10-24\\nContent: pointing to potential areas for further exploration. Github:\\nhttps://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.\\n']\n",
            "‚úçÔ∏è Writing summary for research task:  What new research work was published in the past 30 days on image generation especially on disfussion based model?...\n",
            "\u001b[32m# Recent Research on Diffusion-Based Image Generation Models\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m## Introduction\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mIn the past 30 days, several significant research works have been published focusing on image generation, particularly on diffusion-based models. This report aims to provide a detailed, comprehensive, and insightful overview of these recent advancements. The current date is June 11, 2024, and the report will cover publications from May 11, 2024, to June 11, 2024.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m## Recent Publications\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m### Flash Diffusion: Accelerating Pre-trained Diffusion Models\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mOne of the notable recent works is the introduction of Flash Diffusion, a method designed to accelerate the generation of pre-trained diffusion models. This method achieves state-of-the-art performance in terms of Fr√©chet Inception Distance (FID) and CLIP-Score for few-step image generation on the COCO2014 and COCO2017 datasets. Flash Diffusion requires only several GPU hours of training and fewer trainable parameters than existing methods. It demonstrates versatility across various tasks such as text-to-image, inpainting, face-swapping, and super-resolution, using different backbones like UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-Œ±) ([arxiv.org](https://arxiv.org/abs/2406.02347)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m### Ouroboros3D: Unified 3D Generation Framework\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mAnother significant publication is the introduction of Ouroboros3D, a unified 3D generation framework that integrates diffusion-based multi-view image generation and 3D reconstruction into a recursive diffusion process. This framework addresses the data bias issue in the inference phase by jointly training the multi-view image generation and 3D reconstruction modules through a self-conditioning mechanism. The recursive diffusion framework with 3D-aware feedback improves geometric consistency and outperforms existing methods that separate these stages ([arxiv.org](https://arxiv.org/pdf/2406.03184)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m### Multistep Consistency Models\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mA new approach called Multistep Consistency Models has been proposed, which unifies Consistency Models and TRACT. This method allows for a trade-off between sampling speed and sampling quality. By increasing the sample budget from a single step to 2-8 steps, the models can generate higher quality samples while retaining much of the sampling speed benefits. Notable results include a 1.4 FID on Imagenet 64 in 8 steps and 2.1 FID on Imagenet128 in 8 steps with consistency distillation ([arxiv.org](https://arxiv.org/pdf/2403.06807)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m## Analysis and Insights\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m### Efficiency and Versatility\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mThe recent advancements in diffusion-based models highlight a trend towards improving efficiency and versatility. Flash Diffusion, for instance, not only reduces the computational time required for image generation but also maintains high-quality outputs across various tasks. This is particularly important for applications that require quick and reliable image generation, such as real-time visual editing and industrial-scale text-to-image generation.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m### Integration of Multi-View and 3D Reconstruction\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mThe Ouroboros3D framework represents a significant step forward in integrating multi-view image generation and 3D reconstruction. By addressing the data bias issue and improving geometric consistency, this framework enhances the quality of 3D-generated images, which is crucial for applications in fields like virtual reality and medical imaging.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m### Trade-Off Between Speed and Quality\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mThe introduction of Multistep Consistency Models provides a flexible approach to balancing sampling speed and quality. This method allows researchers and practitioners to adjust the number of steps based on the specific requirements of their applications, making it a valuable tool for a wide range of image generation tasks.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m## Conclusion\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mThe past 30 days have seen significant advancements in diffusion-based image generation models, with a focus on improving efficiency, versatility, and the balance between speed and quality. These developments are poised to have a substantial impact on various applications, from real-time visual editing to 3D reconstruction and beyond.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m## References\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m- [arxiv.org](https://arxiv.org/abs/2406.02347)\n",
            "\u001b[0m\n",
            "\u001b[32m- [arxiv.org](https://arxiv.org/pdf/2406.03184)\n",
            "\u001b[0m\n",
            "# Recent Research on Diffusion-Based Image Generation Models\n",
            "\n",
            "## Introduction\n",
            "\n",
            "In the past 30 days, several significant research works have been published focusing on image generation, particularly on diffusion-based models. This report aims to provide a detailed, comprehensive, and insightful overview of these recent advancements. The current date is June 11, 2024, and the report will cover publications from May 11, 2024, to June 11, 2024.\n",
            "\n",
            "## Recent Publications\n",
            "\n",
            "### Flash Diffusion: Accelerating Pre-trained Diffusion Models\n",
            "\n",
            "One of the notable recent works is the introduction of Flash Diffusion, a method designed to accelerate the generation of pre-trained diffusion models. This method achieves state-of-the-art performance in terms of Fr√©chet Inception Distance (FID) and CLIP-Score for few-step image generation on the COCO2014 and COCO2017 datasets. Flash Diffusion requires only several GPU hours of training and fewer trainable parameters than existing methods. It demonstrates versatility across various tasks such as text-to-image, inpainting, face-swapping, and super-resolution, using different backbones like UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-Œ±) ([arxiv.org](https://arxiv.org/abs/2406.02347)).\n",
            "\n",
            "### Ouroboros3D: Unified 3D Generation Framework\n",
            "\n",
            "Another significant publication is the introduction of Ouroboros3D, a unified 3D generation framework that integrates diffusion-based multi-view image generation and 3D reconstruction into a recursive diffusion process. This framework addresses the data bias issue in the inference phase by jointly training the multi-view image generation and 3D reconstruction modules through a self-conditioning mechanism. The recursive diffusion framework with 3D-aware feedback improves geometric consistency and outperforms existing methods that separate these stages ([arxiv.org](https://arxiv.org/pdf/2406.03184)).\n",
            "\n",
            "### Multistep Consistency Models\n",
            "\n",
            "A new approach called Multistep Consistency Models has been proposed, which unifies Consistency Models and TRACT. This method allows for a trade-off between sampling speed and sampling quality. By increasing the sample budget from a single step to 2-8 steps, the models can generate higher quality samples while retaining much of the sampling speed benefits. Notable results include a 1.4 FID on Imagenet 64 in 8 steps and 2.1 FID on Imagenet128 in 8 steps with consistency distillation ([arxiv.org](https://arxiv.org/pdf/2403.06807)).\n",
            "\n",
            "## Analysis and Insights\n",
            "\n",
            "### Efficiency and Versatility\n",
            "\n",
            "The recent advancements in diffusion-based models highlight a trend towards improving efficiency and versatility. Flash Diffusion, for instance, not only reduces the computational time required for image generation but also maintains high-quality outputs across various tasks. This is particularly important for applications that require quick and reliable image generation, such as real-time visual editing and industrial-scale text-to-image generation.\n",
            "\n",
            "### Integration of Multi-View and 3D Reconstruction\n",
            "\n",
            "The Ouroboros3D framework represents a significant step forward in integrating multi-view image generation and 3D reconstruction. By addressing the data bias issue and improving geometric consistency, this framework enhances the quality of 3D-generated images, which is crucial for applications in fields like virtual reality and medical imaging.\n",
            "\n",
            "### Trade-Off Between Speed and Quality\n",
            "\n",
            "The introduction of Multistep Consistency Models provides a flexible approach to balancing sampling speed and quality. This method allows researchers and practitioners to adjust the number of steps based on the specific requirements of their applications, making it a valuable tool for a wide range of image generation tasks.\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "The past 30 days have seen significant advancements in diffusion-based image generation models, with a focus on improving efficiency, versatility, and the balance between speed and quality. These developments are poised to have a substantial impact on various applications, from real-time visual editing to 3D reconstruction and beyond.\n",
            "\n",
            "## References\n",
            "\n",
            "- [arxiv.org](https://arxiv.org/abs/2406.02347)\n",
            "- [arxiv.org](https://arxiv.org/pdf/2406.03184)\n",
            "- [arxiv.org](https://arxiv.org/pdf/2403.06807)\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio # required for notebooks\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from gpt_researcher import GPTResearcher\n",
        "import asyncio\n",
        "#, config_path=\"config.json\"#\n",
        "async def get_report(query: str, report_type: str) -> str:\n",
        "    researcher = GPTResearcher(query, report_type,only_arxiv=False,verbose=True)\n",
        "    await researcher.conduct_research()\n",
        "    report = await researcher.write_report()\n",
        "    return report\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = \" What new research work was published in the past 3 months on image generation especially on disfussion based model?\"\n",
        "    report_type = \"research_report\"\n",
        "\n",
        "    report = asyncio.run(get_report(query, report_type))\n",
        "    print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\Conda\\envs\\gptresearch\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.retrievers import ArxivRetriever\n",
        "\n",
        "query = 'DIFUSSION model'\n",
        "retriever = ArxivRetriever(load_max_docs=5, doc_content_chars_max=1024)\n",
        "docs = retriever.get_relevant_documents(query=query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In this paper, we study the relationship between a diffusive model and a\\nnon-diffusive model which are both derived from the well-known Keller-Segel\\nmodel, as a coefficient of diffusion $\\\\varepsilon$ goes to zero. First, we\\nestablish the global well-posedness of classical solutions to the Cauchy\\nproblem for the diffusive model with smooth initial data which is of small\\n$L^2$ norm, together with some {\\\\it a priori} estimates uniform for $t$ and\\n$\\\\varepsilon$. Then we investigate the zero-diffusion limit, and get the global\\nwell-posedness of classical solutions to the Cauchy problem for the\\nnon-diffusive model. Finally, we derive the convergence rate of the diffusive\\nmodel toward the non-diffusive model. It is shown that the convergence rate in\\n$L^\\\\infty$ norm is of the order $O(\\\\varepsilon^{1/2})$. It should be noted that\\nthe initial data is small in $L^2$-norm but can be of large oscillations with\\nconstant state at far field. As a byproduct, we improve the corresponding\\nresult on the well-posedness of the non-difussive model which requires small\\noscillations.'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='In this paper, we study the relationship between a diffusive model and a\\nnon-diffusive model which are both derived from the well-known Keller-Segel\\nmodel, as a coefficient of diffusion $\\\\varepsilon$ goes to zero. First, we\\nestablish the global well-posedness of classical solutions to the Cauchy\\nproblem for the diffusive model with smooth initial data which is of small\\n$L^2$ norm, together with some {\\\\it a priori} estimates uniform for $t$ and\\n$\\\\varepsilon$. Then we investigate the zero-diffusion limit, and get the global\\nwell-posedness of classical solutions to the Cauchy problem for the\\nnon-diffusive model. Finally, we derive the convergence rate of the diffusive\\nmodel toward the non-diffusive model. It is shown that the convergence rate in\\n$L^\\\\infty$ norm is of the order $O(\\\\varepsilon^{1/2})$. It should be noted that\\nthe initial data is small in $L^2$-norm but can be of large oscillations with\\nconstant state at far field. As a byproduct, we improve the corresponding\\nresult on the well-posedness of the non-difussive model which requires small\\noscillations.', metadata={'Entry ID': 'http://arxiv.org/abs/1210.5101v1', 'Published': datetime.date(2012, 10, 18), 'Title': 'Global well-posedness and zero-diffusion limit of classical solutions to the 3D conservation laws arising in chemotaxis', 'Authors': 'Hongyun Peng, Huanyao Wen, Changjiang Zhu'}),\n",
              " Document(page_content='Diffusion MRI (dMRI) is an important neuroimaging technique with high\\nacquisition costs. Deep learning approaches have been used to enhance dMRI and\\npredict diffusion biomarkers through undersampled dMRI. To generate more\\ncomprehensive raw dMRI, generative adversarial network based methods are\\nproposed to include b-values and b-vectors as conditions, but they are limited\\nby unstable training and less desirable diversity. The emerging diffusion model\\n(DM) promises to improve generative performance. However, it remains\\nchallenging to include essential information in conditioning DM for more\\nrelevant generation, i.e., the physical principles of dMRI and white matter\\ntract structures. In this study, we propose a physics-guided diffusion model to\\ngenerate high-quality dMRI. Our model introduces the physical principles of\\ndMRI in the noise evolution in the diffusion process and introduce a\\nquery-based conditional mapping within the difussion model. In addition, to\\nenhance the anatomical fine detials of the generation, we introduce the XTRACT\\natlas as prior of white matter tracts by adopting an adapter technique. Our\\nexperiment results show that our method outperforms other state-of-the-art\\nmethods and has the potential to advance dMRI enhancement.', metadata={'Entry ID': 'http://arxiv.org/abs/2406.03002v1', 'Published': datetime.date(2024, 6, 5), 'Title': 'Phy-Diff: Physics-guided Hourglass Diffusion Model for Diffusion MRI Synthesis', 'Authors': 'Juanhua Zhang, Ruodan Yan, Alessandro Perelli, Xi Chen, Chao Li'}),\n",
              " Document(page_content='We present a complete study of the Multiparticle Biased Diffusion-Limited\\nAggregation (MBDLA) model supplemented with surface difussion (SD), focusing on\\nthe relevance and effects of the latter transport mechanism. By comparing\\ndifferent algorithms, we show that MBDLA+SD is a very good qualitative model\\nfor electrodeposition in practically all the range of current intensities {\\\\em\\nprovided} one introduces SD in the model in the proper fashion: We have found\\nthat the correct procedure involves simultaneous bulk diffusion and SD,\\nintroducing a time scale arising from the ratio of the rates of both processes.\\nWe discuss in detail the different morphologies obtained and compare them to\\nthe available experimental data with very satisfactory results. We also\\ncharacterize the aggregates thus obtained by means of the dynamic scaling\\nexponents of the interface height, allowing us to distinguish several regimes\\nin the mentioned interface growth. Our asymptotic scaling exponents are again\\nin good agreement with recent experiments. We conclude by discussing a global\\npicture of the influence and consequences of SD in electrodeposition.', metadata={'Entry ID': 'http://arxiv.org/abs/cond-mat/0003167v1', 'Published': datetime.date(2000, 3, 10), 'Title': 'Multiparticle Biased DLA with surface diffusion: a comprehensive model of electrodeposition', 'Authors': 'Mario Castro, Rodolfo Cuerno, Angel Sanchez, Francisco Dominguez-Adame'})]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "return docs[0].page_content, docs[0].metadata.get('Published') "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
